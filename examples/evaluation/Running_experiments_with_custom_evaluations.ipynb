{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running experiments with custom evaluations\n",
    "\n",
    "We illustrate how we can run custom scorers to perform automatic evaluations of responses when sending a prompt to an API. We will use the Anthropic API to query a model and evaluate the results with a custom evaluation function, however, feel free to adapt the provided input experiment file to use another API.\n",
    "\n",
    "In the [evaluation docs](https://alan-turing-institute.github.io/prompto/docs/evaluation/#automatic-evaluation-using-a-scoring-function), we provide an explanation of scoring functions and how they can be applied to evaluate responses from models. In this notebook, we will show how to use a custom scorer to evaluate responses from a model in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompto.settings import Settings\n",
    "from prompto.experiment import Experiment\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "In this experiment, we will use the Anthropic API, but feel free to edit the input file provided to use a different API and model.\n",
    "\n",
    "When using `prompto` to query models from the Anthropic API, lines in our experiment `.jsonl` files must have `\"api\": \"anthropic\"` in the prompt dict. \n",
    "\n",
    "For the [Anthropic API](https://alan-turing-institute.github.io/prompto/docs/anthropic/), there are two environment variables that could be set:\n",
    "- `ANTHROPIC_API_KEY`: the API key for the Anthropic API\n",
    "\n",
    "As mentioned in the [environment variables docs](https://alan-turing-institute.github.io/prompto/docs/environment_variables/#model-specific-environment-variables), there are also model-specific environment variables too which can be utilised. In particular, when you specify a `model_name` key in a prompt dict, one could also specify a `ANTHROPIC_API_KEY_model_name` environment variable to indicate the API key used for that particular model (where \"model_name\" is replaced to whatever the corresponding value of the `model_name` key is). We will see a concrete example of this later.\n",
    "\n",
    "To set environment variables, one can simply have these in a `.env` file which specifies these environment variables as key-value pairs:\n",
    "```\n",
    "ANTHROPIC_API_KEY=<YOUR-ANTHROPIC-KEY>\n",
    "```\n",
    "\n",
    "If you make this file, you can run the following which should return `True` if it's found one, or `False` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we obtain those values. We raise an error if the `ANTHROPIC_API_KEY` environment variable hasn't been set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "if ANTHROPIC_API_KEY is None:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get any errors or warnings in the above two cells, try to fix your `.env` file like the example we have above to get these variables set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a custom evaluation function\n",
    "\n",
    "The only rule when writing custom evaluations is that the function should take in a single argument which is the `prompt_dict` with the responses from the API. The function should return the same dictionary with any additional keys that you want to add.\n",
    "\n",
    "In the following example, this is not a particularly useful evaluation in most cases - it simply performs a rough word count of the response by splitting on spaces. In a real-world scenario, you might want to compare it to some reference text (which could be provided in the prompt dictionary as an \"expected_response\" key) or use a more sophisticated evaluation, e.g. some regex computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_response(response_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    This function is an example of an evaluation function that can be used to evaluate the response of an experiment.\n",
    "    It counts the number of words in the response and adds it to the response_dict. It also adds a boolean value to\n",
    "    the response_dict that is True if the response has more than 10 words and False otherwise.\n",
    "    \"\"\"\n",
    "    # Count the number of spaces in the response\n",
    "    response_dict[\"word_count\"] = response_dict[\"response\"].count(\" \") + 1\n",
    "    response_dict[\"more_than_10_words\"] = response_dict[\"word_count\"] > 10\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply run the experiment in the same way as normal, but pass in your evaluation function into `process` method of the `Experiment` object.\n",
    "\n",
    "Note more than one functions can be passed and they will be executed in the order they are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(data_folder=\"./data\", max_queries=30)\n",
    "experiment = Experiment(file_name=\"input-evaluation.jsonl\", settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 2 queries at 30 QPM with RI of 2.0s  (attempt 1/3): 100%|██████████| 2/2 [00:04<00:00,  2.00s/query]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 2/2 [00:01<00:00,  1.09query/s]\n"
     ]
    }
   ],
   "source": [
    "responses, avg_query_processing_time = await experiment.process(\n",
    "    evaluation_funcs=[count_words_in_response]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'api': 'anthropic',\n",
       "  'model_name': 'claude-3-5-sonnet-20240620',\n",
       "  'prompt': 'How does technology impact us? Keep the response to less than 10 words.',\n",
       "  'parameters': {'temperature': 1, 'max_tokens': 100},\n",
       "  'timestamp_sent': '30-08-2024-08-58-02',\n",
       "  'response': 'Technology revolutionizes communication, work, and daily life, reshaping human experiences.',\n",
       "  'Word Count': 10,\n",
       "  'more_than_10_words': False},\n",
       " {'id': 0,\n",
       "  'api': 'anthropic',\n",
       "  'model_name': 'claude-3-haiku-20240307',\n",
       "  'prompt': 'How does technology impact us?',\n",
       "  'parameters': {'temperature': 1, 'max_tokens': 100},\n",
       "  'timestamp_sent': '30-08-2024-08-58-00',\n",
       "  'response': 'Technology has had a profound impact on our lives in both positive and negative ways. Here are some of the key ways technology has influenced us:\\n\\nPositive impacts:\\n- Increased connectivity and communication - Technology has made it easier to stay in touch with loved ones, coordinate with colleagues, and access information.\\n- Advancements in healthcare - Medical technologies have led to longer lifespans, new treatments, and better disease prevention.\\n- Improved productivity and efficiency - Many jobs an',\n",
       "  'Word Count': 75,\n",
       "  'more_than_10_words': True}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.completed_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results from the evaluation function in the completed responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a scorer automatically from the command line\n",
    "\n",
    "In the [evaluation docs](https://alan-turing-institute.github.io/prompto/docs/evaluation/#running-a-scorer-evaluation-automatically-using-prompto_run_experiment), we discuss how you can use the `prompto_run_experiment` command line tool to run experiments and automatically evaluate responses using a scorer.\n",
    "\n",
    "In this case, we would need to define the above function in a Python file and add it to the `SCORING_FUNCTIONS` dictionary in the [src/prompto/scorers.py](https://github.com/alan-turing-institute/prompto/blob/main/src/prompto/scorer.py) file. We could add the following key and value to the dictionary:\n",
    "    \n",
    "```python\n",
    "\"count_words_in_response\": count_words_in_response\n",
    "```\n",
    "\n",
    "Then, we could run the following command to run the experiment and evaluate the responses using the custom scorer:\n",
    "```bash\n",
    "prompto_run_experiment --file <path-to-experiment-file> --scorer count_words_in_response\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
