{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rephrasing prompts using `prompto`\n",
    "\n",
    "We illustrate how we can use `prompto` to rephrase prompts. This is useful if you first want to generate a more diverse set of prompts and then use them to generate a more diverse set of completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompto.settings import Settings\n",
    "from prompto.experiment import Experiment\n",
    "from prompto.rephrasal import Rephraser, load_rephrase_folder\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `prompto` to query models from the Ollama API, lines in our experiment `.jsonl` files must have `\"api\": \"ollama\"` in the prompt dict. \n",
    "\n",
    "## Setting up Ollama locally\n",
    "\n",
    "In this notebook, we assume that you have a local instance of the Ollama API running. For installing Ollama, please refer to the [Ollama documentation](https://ollama.com/). Once you have it installed and have it running, e.g. with `ollama serve` in the terminal, you can proceed with the following steps.\n",
    "\n",
    "\n",
    "By default, the address and port that Ollama uses when running is `localhost:11434`. When developing this notebook, we were running Ollama locally so we set the `OLLAMA_API_ENDPOINT` to `http://localhost:11434`. If you are running the server at a different address or port, you can specify with the `OLLAMA_API_ENDPOINT` environment variable accordingly as described below.\n",
    "\n",
    "### Downloading models\n",
    "\n",
    "In this notebook and our example experiment file ([example_file.jsonl](https://github.com/alan-turing-institute/prompto/tree/main/examples/evaluation/example_file.jsonl)), we have set to query from Gemma2 models and we will first rephrase these prompts using Llama 3.2 - note that Ollama defaults to the smaller versions of these (8B, 2B). You can download these models using the following commands in the terminal:\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "ollama pull gemma2\n",
    "```\n",
    "\n",
    "If you'd prefer to query other models, you can replace the model names in the experiment file with the models you have downloaded. We simply return an error if the model is not found in the Ollama endpoint that is running.\n",
    "\n",
    "### Environment variables\n",
    "\n",
    "For the [Ollama API](https://alan-turing-institute.github.io/prompto/docs/ollama/), there are two environment variables that could be set:\n",
    "- `OLLAMA_API_ENDPOINT`: the API endpoint for the Ollama API\n",
    "\n",
    "As mentioned in the [environment variables docs](https://alan-turing-institute.github.io/prompto/docs/environment_variables/#model-specific-environment-variables), there are also model-specific environment variables too which can be utilised. In particular, if you specify a `model_name` key in a prompt dict, one could also specify a `OLLAMA_API_ENDPOINT_model_name` environment variable to indicate the API key used for that particular model (where \"model_name\" is replaced to whatever the corresponding value of the `model_name` key is). We will see a concrete example of this later.\n",
    "\n",
    "To set environment variables, one can simply have these in a `.env` file which specifies these environment variables as key-value pairs:\n",
    "```\n",
    "OLLAMA_API_ENDPOINT=<YOUR-OLLAMA-ENDPOINT>\n",
    "```\n",
    "\n",
    "If you make this file, you can run the following which should return `True` if it's found one, or `False` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we obtain those values. We raise an error if the `OLLAMA_API_ENDPOINT` environment variable hasn't been set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OLLAMA_API_ENDPOINT: http://localhost:11434\n"
     ]
    }
   ],
   "source": [
    "OLLAMA_API_ENDPOINT = os.environ.get(\"OLLAMA_API_ENDPOINT\")\n",
    "if OLLAMA_API_ENDPOINT is None:\n",
    "    raise ValueError(\"OLLAMA_API_ENDPOINT is not set\")\n",
    "else:\n",
    "    print(f\"Using OLLAMA_API_ENDPOINT: {OLLAMA_API_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get any errors or warnings in the above two cells, try to fix your `.env` file like the example we have above to get these variables set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Rephraser` class\n",
    "\n",
    "The `Rephraser` class is a class that can be used to generate rephrased/paraphrased versions of a given prompt. To initialise the `Rephraser` class, we need to provide the following arguments:\n",
    "- `input_prompts`: a list of input prompt dictionaries (a prompt dictionary with a `\"prompt\"` key along with the other standard keys like `\"id\"`, `\"api\"`, `\"model_name\"`, etc.) - this can just be read in from an input `.jsonl` file\n",
    "- `template_prompts`: a list of templates to use for rephrasing the input prompts. There should be `{INPUT_PROMPT}` placeholders for which the prompt will be inserted\n",
    "- `template_settings`: a dictionary where the keys are the identifiers for a particular model for rephrasal and the values are also dictionaries containing the `\"api\"`, `\"model_name\"`, and `\"parameters\"` to specify the LLM to use for rephrasal\n",
    "\n",
    "Typically, `template_prompts` and `template_settings` are stored in a `rephrase` folder (see the [rephrasals documentation](https://alan-turing-institute.github.io/prompto/docs/rephrasals/#rephrase-folder) for more details), which we can simply load using the `load_rephrase_folder` function from `prompto`.\n",
    "\n",
    "We provide an example of such folder [here](https://github.com/alan-turing-institute/prompto/tree/main/examples/evaluation/rephrase).\n",
    "\n",
    "To use `load_rephrase_folder`, we simply pass in the path to the folder and a list of template `.txt` files that we want to load. Here `template.txt` is a file in `./rephrase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_prompts, rephrase_settings = load_rephrase_folder(\n",
    "    \"./rephrase\", templates=\"template.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the prompt templates have been loaded as a list of strings from `template.txt` where each line from that file is a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Write a paraphrase for the following sentence. Only reply with the paraphrased prompt: \"{INPUT_PROMPT}\"',\n",
       " 'Write a variation of this sentence (only reply with the variation): \"{INPUT_PROMPT}\"',\n",
       " 'How would you say the following sentence in a different way? Only reply with the different way: \"{INPUT_PROMPT}\"',\n",
       " 'Rewrite the following task instruction. Just reply with the rewritten task. Make sure to keep the task the same, but vary the wording and setting.\\n\"{INPUT_PROMPT}\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, these have placeholder `{INPUT_PROMPT}` which will be replaced with the input prompt from the input prompt dictionaries.\n",
    "\n",
    "Looking at the rephrase settings, we have given some examples of models that we might want to use for rephrasals which are given a identifier as the key name and the value is a dictionary with the keys `\"api\"`, `\"model_name\"`, and `\"parameters\"` specifying where the model is from, the model name, and the parameters to use for the model respectively. We only have one here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ollama-llama3-2': {'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephrase_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in an experiment file [here](https://github.com/alan-turing-institute/prompto/tree/main/examples/evaluation/example_file.jsonl) which we load in as a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./example_file.jsonl\", \"r\") as f:\n",
    "    input_prompts = [dict(json.loads(line)) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'prompt': 'Where can I buy a can of coke?'},\n",
       " {'id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'prompt': 'Who won the champions league in the year 2008?'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialise the `Rephraser` class with the input prompts, template prompts, and template settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rephraser = Rephraser(\n",
    "    input_prompts=input_prompts,\n",
    "    template_prompts=template_prompts,\n",
    "    rephrase_settings=rephrase_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create some prompts to a model for rephrasing using the `create_rephrased_prompts` method. This method just takes in a single argument `rephrase_model` which is the identifier for the model we want to use for rephrasing. This has to be a key in the `rephrase_settings` dictionary we passed in during initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '0': 100%|██████████| 2/2 [00:00<00:00, 25040.62inputs/s]\n",
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '1': 100%|██████████| 2/2 [00:00<00:00, 47662.55inputs/s]\n",
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '2': 100%|██████████| 2/2 [00:00<00:00, 60787.01inputs/s]\n",
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '3': 100%|██████████| 2/2 [00:00<00:00, 66576.25inputs/s]\n"
     ]
    }
   ],
   "source": [
    "rephrase_inputs = rephraser.create_rephrase_inputs(rephrase_model=\"ollama-llama3-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we have two input prompts and 4 templates, we should expect 8 rephrased prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rephrase_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this an experiment file to run, we can simply write this to a `.jsonl` file, but we have a `create_rephrase_file` method to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '0': 100%|██████████| 2/2 [00:00<00:00, 16644.06inputs/s]\n",
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '1': 100%|██████████| 2/2 [00:00<00:00, 52428.80inputs/s]\n",
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '2': 100%|██████████| 2/2 [00:00<00:00, 55924.05inputs/s]\n",
      "Creating rephrase inputs for rephrase model 'ollama-llama3-2' and template '3': 100%|██████████| 2/2 [00:00<00:00, 62601.55inputs/s]\n",
      "Writing rephrase prompts to ./data/input/rephrase-example.jsonl: 100%|██████████| 8/8 [00:00<00:00, 63072.24prompts/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'rephrase-ollama-llama3-2-0-coke_question',\n",
       "  'template_index': 0,\n",
       "  'prompt': 'Write a paraphrase for the following sentence. Only reply with the paraphrased prompt: \"Where can I buy a can of coke?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'coke_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Where can I buy a can of coke?'},\n",
       " {'id': 'rephrase-ollama-llama3-2-0-champions_league_question',\n",
       "  'template_index': 0,\n",
       "  'prompt': 'Write a paraphrase for the following sentence. Only reply with the paraphrased prompt: \"Who won the champions league in the year 2008?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'champions_league_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?'},\n",
       " {'id': 'rephrase-ollama-llama3-2-1-coke_question',\n",
       "  'template_index': 1,\n",
       "  'prompt': 'Write a variation of this sentence (only reply with the variation): \"Where can I buy a can of coke?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'coke_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Where can I buy a can of coke?'},\n",
       " {'id': 'rephrase-ollama-llama3-2-1-champions_league_question',\n",
       "  'template_index': 1,\n",
       "  'prompt': 'Write a variation of this sentence (only reply with the variation): \"Who won the champions league in the year 2008?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'champions_league_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?'},\n",
       " {'id': 'rephrase-ollama-llama3-2-2-coke_question',\n",
       "  'template_index': 2,\n",
       "  'prompt': 'How would you say the following sentence in a different way? Only reply with the different way: \"Where can I buy a can of coke?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'coke_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Where can I buy a can of coke?'},\n",
       " {'id': 'rephrase-ollama-llama3-2-2-champions_league_question',\n",
       "  'template_index': 2,\n",
       "  'prompt': 'How would you say the following sentence in a different way? Only reply with the different way: \"Who won the champions league in the year 2008?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'champions_league_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?'},\n",
       " {'id': 'rephrase-ollama-llama3-2-3-coke_question',\n",
       "  'template_index': 3,\n",
       "  'prompt': 'Rewrite the following task instruction. Just reply with the rewritten task. Make sure to keep the task the same, but vary the wording and setting.\\n\"Where can I buy a can of coke?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'coke_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Where can I buy a can of coke?'},\n",
       " {'id': 'rephrase-ollama-llama3-2-3-champions_league_question',\n",
       "  'template_index': 3,\n",
       "  'prompt': 'Rewrite the following task instruction. Just reply with the rewritten task. Make sure to keep the task the same, but vary the wording and setting.\\n\"Who won the champions league in the year 2008?\"',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3.2',\n",
       "  'parameters': {'temperature': 0},\n",
       "  'input-id': 'champions_league_question',\n",
       "  'input-api': 'ollama',\n",
       "  'input-model_name': 'gemma2',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephraser.create_rephrase_file(\n",
    "    rephrase_model=\"ollama-llama3-2\", out_filepath=\"./data/input/rephrase-example.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `\"api\"` and `\"model\"` keys are set to the values from the `rephrase_settings` dictionary we passed in during initialisation as this defines the model we want to use for rephrasal. Each prompt dictionary also has `\"input-api\"`, `\"input-model_name\"` and other things from the original input prompt dictionary too so that we know what model we originally wanted to send that prompt to before rephrasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the rephrasal experiment\n",
    "\n",
    "We can run the rephrasal experiment as usual (see the [Running experiments with prompto](../notebooks/running_experiments.ipynb) notebook for more details on running experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(data_folder=\"./data\", max_queries=30)\n",
    "experiment = Experiment(file_name=\"rephrase-example.jsonl\", settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 8 queries at 30 QPM with RI of 2.0s (attempt 1/3): 100%|██████████| 8/8 [00:16<00:00,  2.00s/query]\n",
      "Waiting for responses (attempt 1/3): 100%|██████████| 8/8 [00:00<00:00, 12.62query/s]\n"
     ]
    }
   ],
   "source": [
    "responses, _ = await experiment.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responses of these prompts should be rephrased versions of the input prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Can I find a Coca-Cola in a local store or supermarket?\"',\n",
       " '\"Which team claimed the Champions League title in 2008?\"',\n",
       " '\"Can I purchase a cold can of Coca-Cola at your convenience?\"',\n",
       " '\"Which team claimed the Champions League title in 2008?\"',\n",
       " '\"Can I purchase a Coca-Cola from around here?\"',\n",
       " '\"Which team lifted the Champions League trophy that year?\"',\n",
       " '\"In what retail establishment or convenience store can I procure a single serving of Coca-Cola in a glass bottle?\"',\n",
       " '\"What was the victor of the prestigious European club football competition in the calendar year 2008?\"']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[\"response\"] for x in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and running a new rephrased input file\n",
    "\n",
    "We can create a new input file with the rephrased prompts with the `create_new_input_file` method. This method takes in a list of completed responses from the rephrasing experiment. We create a new input file where we send prompts to the original API and model we wanted to send to before rephrasing.\n",
    "\n",
    "Note there is also a `keep_original` argument. If this is True, the original prompts are kept in the new input file. If False, the original prompts are not included and so only the rephrased prompts are in the new input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing new input prompts to ./data/input/post-rephrase-example.jsonl: 100%|██████████| 10/10 [00:00<00:00, 85423.71prompts/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'rephrase-ollama-llama3-2-0-coke_question',\n",
       "  'prompt': '\"Can I find a Coca-Cola in a local store or supermarket?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'rephrase-ollama-llama3-2-0-champions_league_question',\n",
       "  'prompt': '\"Which team claimed the Champions League title in 2008?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'rephrase-ollama-llama3-2-1-coke_question',\n",
       "  'prompt': '\"Can I purchase a cold can of Coca-Cola at your convenience?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'rephrase-ollama-llama3-2-1-champions_league_question',\n",
       "  'prompt': '\"Which team claimed the Champions League title in 2008?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'rephrase-ollama-llama3-2-2-coke_question',\n",
       "  'prompt': '\"Can I purchase a Coca-Cola from around here?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'rephrase-ollama-llama3-2-2-champions_league_question',\n",
       "  'prompt': '\"Which team lifted the Champions League trophy that year?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'rephrase-ollama-llama3-2-3-coke_question',\n",
       "  'prompt': '\"In what retail establishment or convenience store can I procure a single serving of Coca-Cola in a glass bottle?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'rephrase-ollama-llama3-2-3-champions_league_question',\n",
       "  'prompt': '\"What was the victor of the prestigious European club football competition in the calendar year 2008?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2'},\n",
       " {'id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question'},\n",
       " {'id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephraser.create_new_input_file(\n",
    "    keep_original=True,\n",
    "    completed_rephrase_responses=experiment.completed_responses,\n",
    "    out_filepath=\"./data/input/post-rephrase-example.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we have two prompts originally and 8 rephrased prompts, we should expect 10 prompts in the new input file.\n",
    "\n",
    "We can run this rephrased experiment as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrased_experiment = Experiment(\n",
    "    file_name=\"post-rephrase-example.jsonl\", settings=settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 10 queries at 30 QPM with RI of 2.0s (attempt 1/3): 100%|██████████| 10/10 [00:20<00:00,  2.00s/query]\n",
      "Waiting for responses (attempt 1/3): 100%|██████████| 10/10 [00:15<00:00,  1.53s/query]\n"
     ]
    }
   ],
   "source": [
    "rephrased_responses, _ = await rephrased_experiment.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'rephrase-ollama-llama3-2-1-champions_league_question',\n",
       "  'prompt': '\"Which team claimed the Champions League title in 2008?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-32',\n",
       "  'response': 'Manchester United claimed the Champions League title in 2008.  🏆 \\n'},\n",
       " {'id': 'rephrase-ollama-llama3-2-0-champions_league_question',\n",
       "  'prompt': '\"Which team claimed the Champions League title in 2008?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-28',\n",
       "  'response': '**Manchester United** claimed the Champions League title in 2008.  They defeated Chelsea 6-5 on penalties after a 1-1 draw in the final. \\n'},\n",
       " {'id': 'rephrase-ollama-llama3-2-2-champions_league_question',\n",
       "  'prompt': '\"Which team lifted the Champions League trophy that year?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-36',\n",
       "  'response': 'Please tell me which year you are referring to so I can answer your question! 🏆  \\n'},\n",
       " {'id': 'rephrase-ollama-llama3-2-1-coke_question',\n",
       "  'prompt': '\"Can I purchase a cold can of Coca-Cola at your convenience?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-30',\n",
       "  'response': \"As an AI, I don't have a physical body or the ability to interact with the physical world. This means I can't purchase items for you, including a can of Coca-Cola.\\n\\nYou would need to visit a store or use a delivery service to get a cold can of Coca-Cola. 😊 \\n\"},\n",
       " {'id': 'rephrase-ollama-llama3-2-0-coke_question',\n",
       "  'prompt': '\"Can I find a Coca-Cola in a local store or supermarket?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-26',\n",
       "  'response': \"I can't give you real-time information about what's available at your local stores. \\n\\nTo find out if they have Coca-Cola:\\n\\n* **Check the store's website:** Many supermarkets list their inventory online.\\n* **Use a grocery delivery app:** Apps like Instacart or Shipt can tell you which stores near you have Coca-Cola in stock.\\n* **Call the store directly:** This is the most direct way to ask if they have what you need. \\n\\n\\nGood luck finding your Coke! 🥤  \\n\"},\n",
       " {'id': 'rephrase-ollama-llama3-2-3-champions_league_question',\n",
       "  'prompt': '\"What was the victor of the prestigious European club football competition in the calendar year 2008?\"',\n",
       "  'input-prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-40',\n",
       "  'response': 'The victor of the prestigious European club football competition (UEFA Champions League) in the calendar year 2008 was **Manchester United**.  \\n\\nThey defeated Chelsea on penalties after a 1-1 draw in the final held in Moscow, Russia. \\n'},\n",
       " {'id': 'rephrase-ollama-llama3-2-2-coke_question',\n",
       "  'prompt': '\"Can I purchase a Coca-Cola from around here?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-34',\n",
       "  'response': \"As an AI, I don't have access to real-world information like store locations or inventory. \\n\\nTo find out if you can buy a Coca-Cola nearby, I recommend:\\n\\n* **Checking online maps:** Google Maps or Apple Maps can show you nearby convenience stores, grocery stores, and restaurants that likely sell Coca-Cola.\\n* **Using a delivery app:** Apps like Uber Eats, DoorDash, or Grubhub allow you to order food and drinks from local businesses, including Coca-Cola.\\n\\n\\nGood luck finding your Coke! 🥤\"},\n",
       " {'id': 'champions_league_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'prompt': 'Who won the champions league in the year 2008?',\n",
       "  'input-id': 'champions_league_question',\n",
       "  'timestamp_sent': '15-11-2024-12-23-44',\n",
       "  'response': \"**Manchester United** won the Champions League in 2008. \\n\\nThey defeated Chelsea 6-5 on penalties after a 1-1 draw in the final held at Moscow's Luzhniki Stadium. \\n\"},\n",
       " {'id': 'rephrase-ollama-llama3-2-3-coke_question',\n",
       "  'prompt': '\"In what retail establishment or convenience store can I procure a single serving of Coca-Cola in a glass bottle?\"',\n",
       "  'input-prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'timestamp_sent': '15-11-2024-12-23-38',\n",
       "  'response': \"This is tricky! \\n\\nWhile many places sell cans and multi-packs of Coke, finding single glass bottles can be harder.  Here's where you might look:\\n\\n* **Classic Soda Shops:** Look for retro diners or soda fountains - they often have vintage Coke in glass bottles.\\n* **Specialty Stores:** Some gourmet food stores or markets might carry them, especially if they focus on local or regional products.\\n* **Convenience Stores:** While less common, some smaller, independently owned convenience stores might still stock single-serve glass bottles. It depends heavily on your location and the store's inventory.\\n* **Online Retailers:** Sites like Amazon or specialty soda retailers often sell vintage or collectible Coke glass bottles individually.\\n\\n\\nGood luck with your quest for the classic Coke experience! 🥤 \\n\"},\n",
       " {'id': 'coke_question',\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'gemma2',\n",
       "  'prompt': 'Where can I buy a can of coke?',\n",
       "  'input-id': 'coke_question',\n",
       "  'timestamp_sent': '15-11-2024-12-23-42',\n",
       "  'response': \"As an AI, I don't have access to real-time information like store inventories. To find out where you can buy a can of Coke, I recommend:\\n\\n* **Checking nearby convenience stores or gas stations.** These are usually good places to find Coca-Cola products.\\n* **Looking at grocery store websites or apps.** Many grocery stores list their inventory online, so you can check if they have Coke in stock before you go.\\n* **Using a delivery service like Instacart or Uber Eats.** You can order Coke and other groceries to be delivered to your home.\\n\\n\\nHope this helps! \\n\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephrased_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `prompto` from the command line\n",
    "\n",
    "### Creating and running the rephrasal experiment file\n",
    "\n",
    "We can run a rephrasal automatically before when running the experiment by using the `prompto_run_experiment` command:\n",
    "```bash\n",
    "prompto_run_experiment \\\n",
    "    --file example_file.jsonl \\\n",
    "    --max-queries 30 \\\n",
    "    --rephrase-folder rephrase \\\n",
    "    --rephrase-templates template.txt \\\n",
    "    --rephrase-model ollama-llama3-2\n",
    "```\n",
    "\n",
    "This first runs a rephrasal experiment like we saw above and uses those outputs to generate a new input file with rephrased inputs. It will store the final results in a `post-rephrase-example_file` folder in the output folder. The outputs of the rephrase experiment are stored in a `rephrase-example_file` folder in the output folder.\n",
    "\n",
    "There is also a `--remove-original` flag which can be used to remove the original prompts from the new input file (and only have the rephrased prompts).\n",
    "\n",
    "Additionally, there is a `--only-rephrase` flag which indicates that only the rephrasal experiment should be run and no further experiments should be run. In that case, only the rephrasal experiment is run and the outputs are stored in a `rephrase-example_file` folder in the output folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
