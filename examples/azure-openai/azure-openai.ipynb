{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `prompto` with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/prompto/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from prompto.settings import Settings\n",
    "from prompto.experiment import Experiment\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `prompto` to query models from the OpenAI API, lines in our experiment `.jsonl` files must have `\"api\": \"openai\"` in the prompt dict. \n",
    "\n",
    "## Environment variables\n",
    "\n",
    "For the [AzureOpenAI API](../../docs/models.md#azure-openai), there are four environment variables that could be set:\n",
    "- `AZURE_OPENAI_API_KEY`: the API key for the Azure OpenAI API\n",
    "- `AZURE_OPENAI_API_ENDPOINT`: the endpoint for the Azure OpenAI API\n",
    "- `AZURE_OPENAI_API_VERSION`: the version of the Azure OpenAI API (optional)\n",
    "\n",
    "As mentioned in the [model docs](../../docs/models.md#model-specific-environment-variables), there are also model-specific environment variables too which can be utilised. In particular, if you specify a `model_name` key in a prompt dict, one could also specify a `AZURE_OPENAI_API_KEY_model_name` environment variable to indicate the API key used for that particular model (where \"model_name\" is replaced to whatever the corresponding value of the `model_name` key is). We will see a concrete example of this later. The same applies for the `AZURE_OPENAI_API_ENDPOINT_model_name` and `AZURE_OPENAI_API_VERSION_model_name` environment variables.\n",
    "\n",
    "To set environment variables, one can simply have these in a `.env` file which specifies these environment variables as key-value pairs:\n",
    "```\n",
    "AZURE_OPENAI_API_KEY=<YOUR-AZURE-OPENAI-KEY>\n",
    "AZURE_OPENAI_API_ENDPOINT=<YOUR-AZURE-OPENAI-ENDPOINT>\n",
    "AZURE_OPENAI_API_VERSION=<DEFAULT-AZURE-OPENAI-API-VERSION>\n",
    "```\n",
    "\n",
    "If you make this file, you can run the following which should return `True` if it's found one, or `False` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we obtain those values. We raise an error if the `OPENAI_API_KEY` environment variable hasn't been set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "if AZURE_OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_ENDPOINT = os.environ.get(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "if AZURE_OPENAI_API_ENDPOINT is None:\n",
    "    raise ValueError(\"AZURE_OPENAI_API_ENDPOINT is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only raise a warning if `AZURE_OPENAI_API_VERSION` hasn't been set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default AzureOpenAI version: 2024-02-01\n"
     ]
    }
   ],
   "source": [
    "AZURE_OPENAI_API_VERSION = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "if AZURE_OPENAI_API_VERSION is None:\n",
    "    warnings.warn(\"AZURE_OPENAI_API_VERSION is not set\")\n",
    "else:\n",
    "    print(f\"Default AzureOpenAI version: {AZURE_OPENAI_API_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get any errors or warnings in the above two cells, try to fix your `.env` file like the example we have above to get these variables set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of prompts\n",
    "\n",
    "With the OpenAI API, the prompt (given via the `\"prompt\"` key in the prompt dict) can take several forms:\n",
    "- a string: a single prompt to obtain a response for\n",
    "- a list of strings: a sequence of prompts to send to the model\n",
    "    - this is useful in the use case of simulating a conversation with the model by defining the user prompts sequentially\n",
    "- a list of dictionaries with keys \"role\" and \"content\", where \"role\" is one of \"user\", \"assistant\", or \"system\" and \"content\" is the message\n",
    "    - this is useful in the case of passing in some conversation history or to pass in a system prompt to the model\n",
    "\n",
    "We have created an input file in [data/input/azure-openai-example.jsonl](./data/input/azure-openai-example.jsonl) with an example of each of these cases as an illustration.\n",
    "\n",
    "Note that for each of these (besides `\"id\": 1`), we have `\"model_name\": \"reginald-gpt4\"` which refers to a specific GPT-4 deployment that we have on our Azure subscription when developing this notebook. See below for an overview of each of the prompts in the input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(data_folder=\"./data\", max_queries=30)\n",
    "experiment = Experiment(file_name=\"azure-openai-example.jsonl\", settings=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set `max_queries` to 30 so we send 30 queries a minute (every 2 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=./data, max_queries=30, max_attempts=3, parallel=False\n",
      "Subfolders: input_folder=./data/input, output_folder=./data/output, media_folder=./data/media\n"
     ]
    }
   ],
   "source": [
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment.experiment_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the prompts that we have in the `experiment_prompts` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'api': 'azure-openai',\n",
       "  'model_name': 'reginald-gpt4',\n",
       "  'prompt': 'How does technology impact us?',\n",
       "  'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100}},\n",
       " {'id': 1,\n",
       "  'api': 'azure-openai',\n",
       "  'model_name': 'unknown-model-name',\n",
       "  'prompt': 'How does technology impact us?',\n",
       "  'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100}},\n",
       " {'id': 2,\n",
       "  'api': 'azure-openai',\n",
       "  'model_name': 'reginald-gpt4',\n",
       "  'prompt': ['How does international trade create jobs?',\n",
       "   'I want a joke about that'],\n",
       "  'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100}},\n",
       " {'id': 3,\n",
       "  'api': 'azure-openai',\n",
       "  'model_name': 'reginald-gpt4',\n",
       "  'prompt': [{'role': 'system',\n",
       "    'content': 'You are a helpful assistant designed to answer questions briefly.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What efforts are being made to keep the hakka language alive?'}],\n",
       "  'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100}},\n",
       " {'id': 4,\n",
       "  'api': 'azure-openai',\n",
       "  'model_name': 'reginald-gpt4',\n",
       "  'prompt': [{'role': 'system',\n",
       "    'content': 'You are a helpful assistant designed to answer questions briefly.'},\n",
       "   {'role': 'user', 'content': \"Hello, I'm Bob and I'm 6 years old\"},\n",
       "   {'role': 'assistant', 'content': 'Hi Bob, how may I assist you?'},\n",
       "   {'role': 'user', 'content': 'How old will I be next year?'}],\n",
       "  'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.experiment_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the first prompt (`\"id\": 0`), we have a `\"prompt\"` key which is a string.\n",
    "- In the second prompt (`\"id\": 1`), we have a `\"prompt\"` key is also a string but we specify a `\"model_name\"` key to be \"unknown-model-name\". We do this to illustrate what happens when a model name is not recognised as being a deployed model in your Azure subscription - we will see that we get an error for this\n",
    "- In the third prompt (`\"id\": 2`), we have a `\"prompt\"` key which is a list of strings.\n",
    "- In the fourth prompt (`\"id\": 3`), we have a `\"prompt\"` key which is a list of dictionaries. These dictionaries have a \"role\" and \"content\" key. This acts as passing in a system prompt. Here, we just have a system prompt before a user prompt.\n",
    "- In the fifth prompt (`\"id\": 4`), we have a `\"prompt\"` key which is a list of dictionaries. These dictionaries have a \"role\" and \"content\" key. Here, we have a system prompt and a series of user/assistant interactions before finally having a user prompt. This acts as passing in a system prompt and conversation history.\n",
    "\n",
    "Note that for each of these (besides `\"id\": 1`), we have `\"model_name\": \"reginald-gpt4\"` which refers to a specific GPT-4 deployment that we have on our Azure subscription when developing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "\n",
    "We now can run the experiment using the async method `process` which will process the prompts in the input file asynchronously. Note that a new folder named `timestamp-openai-example` (where \"timestamp\" is replaced with the actual date and time of processing) will be created in the output directory and we will move the input file to the output directory. As the responses come in, they will be written to the output file and there are logs that will be printed to the console as well as being written to a log file in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 5 queries  (attempt 1/3): 100%|██████████| 5/5 [00:10<00:00,  2.00s/query]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 5/5 [00:38<00:00,  7.78s/query]\n",
      "Sending 1 queries  (attempt 2/3): 100%|██████████| 1/1 [00:02<00:00,  2.00s/query]\n",
      "Waiting for responses  (attempt 2/3): 100%|██████████| 1/1 [00:00<00:00,  9.00query/s]\n",
      "Sending 1 queries  (attempt 3/3): 100%|██████████| 1/1 [00:02<00:00,  2.00s/query]\n",
      "Waiting for responses  (attempt 3/3): 100%|██████████| 1/1 [00:00<00:00, 10.92query/s]\n"
     ]
    }
   ],
   "source": [
    "responses = await experiment.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the responses are written to the output file, and we can also see them as the returned object. From running the experiment, we obtain prompt dicts where there is now a `\"response\"` key which contains the response(s) from the model.\n",
    "\n",
    "For the case where the prompt is a list of strings, we see that the response is a list of strings where each string is the response to the corresponding prompt.\n",
    "\n",
    "Note here, for our specific Azure subscription, we haven't got a model with deployment name \"gpt-3.5-turbo\" and hence, we actually receive an error message in the response for the second prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'id': 4,\n",
       "   'api': 'azure-openai',\n",
       "   'model_name': 'reginald-gpt4',\n",
       "   'prompt': [{'role': 'system',\n",
       "     'content': 'You are a helpful assistant designed to answer questions briefly.'},\n",
       "    {'role': 'user', 'content': \"Hello, I'm Bob and I'm 6 years old\"},\n",
       "    {'role': 'assistant', 'content': 'Hi Bob, how may I assist you?'},\n",
       "    {'role': 'user', 'content': 'How old will I be next year?'}],\n",
       "   'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100},\n",
       "   'response': 'If you are 6 years old now, you will be 7 years old next year.'},\n",
       "  {'id': 0,\n",
       "   'api': 'azure-openai',\n",
       "   'model_name': 'reginald-gpt4',\n",
       "   'prompt': 'How does technology impact us?',\n",
       "   'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100},\n",
       "   'response': 'Technology impacts nearly every aspect of our lives, often in profound ways. Here is an overview of some of the key areas in which technology exerts its influence:\\n\\n1. Communication:\\n   Technology has revolutionized communication, making it easier, faster, and cheaper. From emails and instant messaging to video calls and social media, technology has brought people closer together, regardless of distance.\\n\\n2. Information Access:\\n   The internet provides access to vast amounts of information on virtually any topic. Search engines, online enc'},\n",
       "  {'id': 3,\n",
       "   'api': 'azure-openai',\n",
       "   'model_name': 'reginald-gpt4',\n",
       "   'prompt': [{'role': 'system',\n",
       "     'content': 'You are a helpful assistant designed to answer questions briefly.'},\n",
       "    {'role': 'user',\n",
       "     'content': 'What efforts are being made to keep the hakka language alive?'}],\n",
       "   'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100},\n",
       "   'response': 'Efforts to keep the Hakka language alive are being undertaken on multiple levels:\\n\\n1. Education: Some schools, particularly in regions with significant Hakka populations like Taiwan, Hong Kong, and parts of China, offer Hakka language courses as part of their curriculum.\\n\\n2. Media and Entertainment: Hakka TV channels, radio stations, and music promote the use of the language through entertainment and news broadcasting.\\n\\n3. Government Policies: Governments in places like Taiwan have policies aimed at preserving minority languages, including'},\n",
       "  {'id': 2,\n",
       "   'api': 'azure-openai',\n",
       "   'model_name': 'reginald-gpt4',\n",
       "   'prompt': ['How does international trade create jobs?',\n",
       "    'I want a joke about that'],\n",
       "   'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100},\n",
       "   'response': ['International trade can create jobs and stimulate economic growth in several ways:\\n\\n1. Export opportunities: When a country exports goods and services to other countries, it can help increase production to meet the demand from international markets. This increase in production often requires additional workers, leading to job creation in various sectors, such as manufacturing, agriculture, and services.\\n\\n2. Import competition and efficiency: Importing goods and services can introduce competition into the domestic market, which can lead to greater efficiency and productivity among domestic firms.',\n",
       "    'Why did the two countries break up before they could start trading with each other?\\n\\nBecause every time they got close, they tariffied each other away!']},\n",
       "  {'id': 1,\n",
       "   'api': 'azure-openai',\n",
       "   'model_name': 'unknown-model-name',\n",
       "   'prompt': 'How does technology impact us?',\n",
       "   'parameters': {'n': 1, 'temperature': 1, 'max_tokens': 100},\n",
       "   'response': \"An unexpected error occurred when querying the API: NotFoundError - Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}} after maximum 3 attempts\"}],\n",
       " 10.623891735076905)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment via the command line\n",
    "\n",
    "We can also run the experiment via the command line. The command is as follows (assuming that your working directory is the current directory of this notebook, i.e. `examples/azure-openai`):\n",
    "```bash\n",
    "prompto_run_experiment --file data/input/azure-openai-example.jsonl --max_queries 30\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
