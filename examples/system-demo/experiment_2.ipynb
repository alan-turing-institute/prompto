{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying different LLM endpoints: `prompto` with parallel processing vs. synchronous Python for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from prompto.settings import Settings\n",
    "from prompto.experiment import Experiment\n",
    "\n",
    "from api_utils import send_prompt\n",
    "from dataset_utils import load_prompt_dicts, load_prompts, generate_experiment_2_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we want to compare the performance of `prompto` which uses asynchronous programming to query model API endpoints with a traditional synchronous Python for loop. For this experiment, we are going to compare the time it takes for `prompto` to obtain 100 responses from different model API endpoints in parallel and the time it takes for a synchronous Python for loop to obtain the same 100 responses from each endpoint.\n",
    "\n",
    "We will see that `prompto` is able to obtain the responses from the different endpoints in parallel, which is much faster than the synchronous Python for loop.\n",
    "\n",
    "As in the [previous experiment](./experiment_1.ipynb), we choose three API endpoints for this experiment:\n",
    "- [OpenAI API](../../docs/openai.md)\n",
    "- [Gemini API](../../docs/gemini.md)\n",
    "- [Ollama API](../../docs/ollama.md) (which is locally hosted)\n",
    "\n",
    "For this experiment, we will need to set up the following environment variables:\n",
    "- `OPENAI_API_KEY`: the API key for the OpenAI API\n",
    "- `GEMINI_API_KEY`: the API key for the Gemini API\n",
    "- `OLLAMA_API_ENDPOINT`: the endpoint for the Ollama API\n",
    "\n",
    "To set these environment variables, one can simply have these in a `.env` file which specifies these environment variables as key-value pairs:\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR-OPENAI=KEY>\n",
    "GEMINI_API_KEY=<YOUR-GEMINI-KEY>\n",
    "OLLAMA_API_ENDPOINT=<YOUR-OLLAMA-ENDPOINT>\n",
    "```\n",
    "\n",
    "If you make this file, you can run the following which should return True if it's found one, or False otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous approach\n",
    "\n",
    "For the synchronous approach, we simply use a for loop to query the API endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompts_sync(prompt_dicts: list[dict]) -> list[str]:\n",
    "    # naive for loop to synchronously dispatch prompts\n",
    "    return [send_prompt(prompt_dict) for prompt_dict in tqdm(prompt_dicts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup\n",
    "\n",
    "For the experiment, we take a sample of 100 prompts from the [`alpaca_data.json`](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json) from the [`tatsu-lab/stanford_alpaca` Github repo](https://github.com/tatsu-lab/stanford_alpaca) and using the prompt template provided by the authors of the repo. To see how we obtain the prompts, please refer to the [alpaca_sample_generation.ipynb](./alpaca_sample_generation.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompts = load_prompts(\"./sample_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create our experiment files using the `generate_experiment_2_file` function in the `dataset_utils.py` file in this directory. This function will just take these prompts and create a jsonl file with the prompts in the format that `prompto` expects. We will save these input files into `./data/input` and use `./data` are our pipeline data folder.\n",
    "\n",
    "See the [pipeline data docs](../../docs/pipeline.md) for more information about the pipeline data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_EXPERIMENT_FILENAME = \"./data/input/all_experiments.jsonl\"\n",
    "\n",
    "INPUT_EXPERIMENT_FILEDIR = \"./data/input\"\n",
    "\n",
    "if not os.path.isdir(INPUT_EXPERIMENT_FILEDIR):\n",
    "    os.mkdir(INPUT_EXPERIMENT_FILEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we query the following models:\n",
    "- `gpt-3.5-turbo` for the OpenAI API\n",
    "- `gemini-1.5-flash` for the Gemini API\n",
    "- `llama3` (8B, 4bit quantised) for the Ollama API\n",
    "\n",
    "Notice that each different API has different argument names for the generation configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_2_file(\n",
    "    path=COMBINED_EXPERIMENT_FILENAME,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=[\"openai\", \"gemini\", \"ollama\"],\n",
    "    model_name=[\"gpt-3.5-turbo\", \"gemini-1.5-flash\", \"llama3\"],\n",
    "    params=[\n",
    "        {\"n\": 1, \"temperature\": 0.9, \"max_tokens\": 100},\n",
    "        {\"candidate_count\": 1, \"temperature\": 0.9, \"max_output_tokens\": 100},\n",
    "        {\"temperature\": 0.9, \"num_predict\": 100, \"seed\": 42},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(load_prompt_dicts(COMBINED_EXPERIMENT_FILENAME)): 300\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"len(load_prompt_dicts(COMBINED_EXPERIMENT_FILENAME)): {len(load_prompt_dicts(COMBINED_EXPERIMENT_FILENAME))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment synchronously\n",
    "\n",
    "Before running the experiment, we will just send an empty prompt request to the Ollama server with the `llama3` model to 1) check that the model is available and working, and 2) to ensure that the model is loaded in memory - sending an empty request in Ollama ensures pre-loading of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    f\"{os.environ.get('OLLAMA_API_ENDPOINT')}/api/generate\", json={\"model\": \"llama3\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `send_prompts_sync` function defined above for the synchronous Python for loop approach. We can run experiments using the `prompto.experiment.Experiment.process` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [09:18<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "multiple_api_sync = send_prompts_sync(\n",
    "    prompt_dicts=load_prompt_dicts(COMBINED_EXPERIMENT_FILENAME)\n",
    ")\n",
    "sync_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment asynchronously with `prompto`\n",
    "\n",
    "We compare the runtime between sending these prompts in a synchronous Python for loop to obtain 100 responses from each API endpoint and using `prompto` with parallel processing. First we will run the synchronous Python for loop and then we will run the `prompto` pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that we are setting `parallel=True` in the `Settings` object as well as specifying the rate limits to send to each of the APIs. As in the [previous experiment](./experiment_1.ipynb), we set the rate limits to 500 queries per minute for OpenAI and Gemini APIs while setting the rate limit to 50 queries per minute for the Ollama API. We do this by passing in a dictionary to the `max_queries_dict` argument in the `Settings` object which has API names as the keys and the rate limits as the values.\n",
    "\n",
    "For details of how to specify rate limits, see the [Specifying rate limits docs](../../docs/rate_limits.md) and the [Grouping prompts and specifying rate limits notebook](../notebooks/grouping_prompts_and_specifying_rate_limits.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for all groups to complete:   0%|          | 0/3 [00:00<?, ?group/s]\n",
      "Sending 100 queries at 500 QPM with RI of 0.12s for group openai  (attempt 1/3): 100%|██████████| 100/100 [00:12<00:00,  8.05query/s]\n",
      "Sending 100 queries at 500 QPM with RI of 0.12s for group gemini  (attempt 1/3): 100%|██████████| 100/100 [00:12<00:00,  8.03query/s]\n",
      "Waiting for responses for group gemini  (attempt 1/3): 100%|██████████| 100/100 [00:01<00:00, 61.42query/s]\n",
      "Waiting for responses for group openai  (attempt 1/3): 100%|██████████| 100/100 [00:01<00:00, 58.39query/s]\n",
      "Sending 100 queries at 50 QPM with RI of 1.2s for group ollama  (attempt 1/3): 100%|██████████| 100/100 [02:00<00:00,  1.21s/query]\n",
      "Waiting for responses for group ollama  (attempt 1/3): 100%|██████████| 100/100 [01:58<00:00,  1.18s/query]\n",
      "Waiting for all groups to complete: 100%|██████████| 3/3 [04:29<00:00, 89.67s/group]\n"
     ]
    }
   ],
   "source": [
    "multiple_api_experiment = Experiment(\n",
    "    file_name=\"all_experiments.jsonl\",\n",
    "    settings=Settings(\n",
    "        data_folder=\"./data\",\n",
    "        parallel=True,\n",
    "        max_queries_dict={\"openai\": 500, \"gemini\": 500, \"ollama\": 50},\n",
    "    ),\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "multiple_api_responses, _ = await multiple_api_experiment.process()\n",
    "prompto_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running `prompto` via the command line\n",
    "\n",
    "We could have also ran the experiments via the command line. The command is as follows (assuming that your working directory is the current directory of this notebook, i.e. `examples/system-demo`):\n",
    "```bash\n",
    "prompto_run_experiment --file data/input/all_experiments.jsonl --parallel True --max-queries-json experiment_2_parallel_config.json\n",
    "```\n",
    "where `experiment_2_parallel_config.json` is a JSON file that specifies the rate limits for each of the API endpoints:\n",
    "```json\n",
    "{\n",
    "    \"openai\": 500,\n",
    "    \"gemini\": 500,\n",
    "    \"ollama\": 50\n",
    "}\n",
    "```\n",
    "\n",
    "But for this notebook, we will time the experiments and save them to the `sync_times` and `prompto_times` dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Here, we report the final runtimes for each API and the difference in time between the `prompto` and synchronous Python for loop approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558.7412779331207, 269.0622651576996)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_time, prompto_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `prompto` approach is much faster than the synchronous Python for loop approach for querying the different model API endpoints. If we compare with the results from the [previous notebook](./experiment_1.ipynb), the `prompto` runtime is very close to just how long it took to process the Ollama requests. This is because the Ollama API has a much longer computation time and we are running at a lower rate limit too. When querying different APIs or models in parallel, you are simply just limited by the slowest API or model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
