{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `prompto` vs. synchronous Python for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from prompto.settings import Settings\n",
    "from prompto.experiment import Experiment\n",
    "\n",
    "from api_utils import send_prompts_sync\n",
    "from dataset_utils import load_prompt_dicts, load_prompts, generate_experiment_1_file\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompts = load_prompts(\"./sample_prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_EXPERIMENT_FILE = \"./data/input/openai.jsonl\"\n",
    "GEMINI_EXPERIMENT_FILE = \"./data/input/gemini.jsonl\"\n",
    "OLLAMA_EXPERIMENT_FILE = \"./data/input/ollama.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=OPENAI_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"openai\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    params={\"n\": 1, \"temperature\": 0.9, \"max_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=GEMINI_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"gemini\",\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    params={\"candidate_count\": 1, \"temperature\": 0.9, \"max_output_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=OLLAMA_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"ollama\",\n",
    "    model_name=\"llama3\",\n",
    "    params={\"temperature\": 0.9, \"num_predict\": 100, \"seed\": 42},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_times = {}\n",
    "prompto_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE)): 50\n",
      "len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE)): 50\n",
      "len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE)): 50\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE)): {len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE))}\"\n",
    ")\n",
    "print(\n",
    "    f\"len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE)): {len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE))}\"\n",
    ")\n",
    "print(\n",
    "    f\"len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE)): {len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "openai_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(OPENAI_EXPERIMENT_FILE))\n",
    "sync_times[\"openai\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 50 queries at 300 QPM with RI of 0.2s  (attempt 1/3): 100%|██████████| 50/50 [00:10<00:00,  4.95query/s]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 50/50 [00:11<00:00,  4.38query/s] \n"
     ]
    }
   ],
   "source": [
    "openai_experiment = Experiment(\n",
    "    file_name=\"openai.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=300)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "openai_responses, _ = await openai_experiment.process()\n",
    "prompto_times[\"openai\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63.54397201538086, 21.541850090026855)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"openai\"], prompto_times[\"openai\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:15<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gemini_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(GEMINI_EXPERIMENT_FILE))\n",
    "sync_times[\"gemini\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 50 queries at 300 QPM with RI of 0.2s  (attempt 1/3): 100%|██████████| 50/50 [00:10<00:00,  4.96query/s]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 50/50 [00:01<00:00, 41.28query/s] \n"
     ]
    }
   ],
   "source": [
    "gemini_experiment = Experiment(\n",
    "    file_name=\"gemini.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=300)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "gemini_responses, _ = await gemini_experiment.process()\n",
    "prompto_times[\"gemini\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75.06720781326294, 11.306013822555542)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"gemini\"], prompto_times[\"gemini\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    f\"{os.environ.get('OLLAMA_API_ENDPOINT')}/api/generate\", json={\"model\": \"llama3\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:14<00:00,  2.68s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ollama_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(OLLAMA_EXPERIMENT_FILE))\n",
    "sync_times[\"ollama\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 50 queries at 40 QPM with RI of 1.5s  (attempt 1/3): 100%|██████████| 50/50 [01:15<00:00,  1.50s/query]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 50/50 [00:56<00:00,  1.13s/query]\n"
     ]
    }
   ],
   "source": [
    "ollama_experiment = Experiment(\n",
    "    file_name=\"ollama.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=40)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "ollama_responses, _ = await ollama_experiment.process()\n",
    "prompto_times[\"ollama\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134.14817905426025, 131.92272996902466)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"ollama\"], prompto_times[\"ollama\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': 63.54397201538086,\n",
       " 'gemini': 75.06720781326294,\n",
       " 'ollama': 134.14817905426025}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': 21.541850090026855,\n",
       " 'gemini': 11.306013822555542,\n",
       " 'ollama': 131.52019500732422}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompto_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
