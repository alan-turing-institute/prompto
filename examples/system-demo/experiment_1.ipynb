{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `prompto` vs. synchronous Python for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from prompto.settings import Settings\n",
    "from prompto.experiment import Experiment\n",
    "\n",
    "from api_utils import send_prompts_sync\n",
    "from dataset_utils import load_prompt_dicts, load_prompts, generate_experiment_1_file\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompts = load_prompts(\"./sample_prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_EXPERIMENT_FILE = \"./data/input/openai.jsonl\"\n",
    "GEMINI_EXPERIMENT_FILE = \"./data/input/gemini.jsonl\"\n",
    "OLLAMA_EXPERIMENT_FILE = \"./data/input/ollama.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=OPENAI_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"openai\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    params={\"n\": 1, \"temperature\": 0.9, \"max_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=GEMINI_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"gemini\",\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    params={\"candidate_count\": 1, \"temperature\": 0.9, \"max_output_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=OLLAMA_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"ollama\",\n",
    "    model_name=\"llama3\",\n",
    "    params={\"temperature\": 0.9, \"num_predict\": 100, \"seed\": 42},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_times = {}\n",
    "prompto_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE)): 100\n",
      "len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE)): 100\n",
      "len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE)): 100\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE)): {len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE))}\"\n",
    ")\n",
    "print(\n",
    "    f\"len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE)): {len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE))}\"\n",
    ")\n",
    "print(\n",
    "    f\"len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE)): {len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "openai_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(OPENAI_EXPERIMENT_FILE))\n",
    "sync_times[\"openai\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 500 QPM with RI of 0.12s  (attempt 1/3): 100%|██████████| 100/100 [00:12<00:00,  8.20query/s]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 100/100 [00:01<00:00, 58.66query/s]\n"
     ]
    }
   ],
   "source": [
    "openai_experiment = Experiment(\n",
    "    file_name=\"openai.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=500)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "openai_responses, _ = await openai_experiment.process()\n",
    "prompto_times[\"openai\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126.30979299545288, 13.91887378692627)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"openai\"], prompto_times[\"openai\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:43<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gemini_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(GEMINI_EXPERIMENT_FILE))\n",
    "sync_times[\"gemini\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 500 QPM with RI of 0.12s  (attempt 1/3):   0%|          | 0/100 [00:00<?, ?query/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 500 QPM with RI of 0.12s  (attempt 1/3): 100%|██████████| 100/100 [00:12<00:00,  8.17query/s]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 100/100 [00:01<00:00, 55.77query/s]\n"
     ]
    }
   ],
   "source": [
    "gemini_experiment = Experiment(\n",
    "    file_name=\"gemini.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=500)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "gemini_responses, _ = await gemini_experiment.process()\n",
    "prompto_times[\"gemini\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163.48729801177979, 14.094270944595337)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"gemini\"], prompto_times[\"gemini\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    f\"{os.environ.get('OLLAMA_API_ENDPOINT')}/api/generate\", json={\"model\": \"llama3\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:31<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ollama_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(OLLAMA_EXPERIMENT_FILE))\n",
    "sync_times[\"ollama\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 40 QPM with RI of 1.5s  (attempt 1/3): 100%|██████████| 100/100 [02:30<00:00,  1.50s/query]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 100/100 [01:58<00:00,  1.18s/query]\n"
     ]
    }
   ],
   "source": [
    "ollama_experiment = Experiment(\n",
    "    file_name=\"ollama.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=40)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "ollama_responses, _ = await ollama_experiment.process()\n",
    "prompto_times[\"ollama\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271.4494206905365, 268.59372997283936)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"ollama\"], prompto_times[\"ollama\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': 126.30979299545288,\n",
       " 'gemini': 163.48729801177979,\n",
       " 'ollama': 271.4494206905365}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': 13.91887378692627,\n",
       " 'gemini': 14.094270944595337,\n",
       " 'ollama': 268.59372997283936}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompto_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
