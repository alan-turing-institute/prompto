{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `prompto` vs. synchronous Python for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from prompto.settings import Settings\n",
    "from prompto.experiment import Experiment\n",
    "\n",
    "from api_utils import send_prompts_sync\n",
    "from dataset_utils import load_prompt_dicts, load_prompts, generate_experiment_1_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we want to compare the performance of `prompto` which uses asynchronous programming to query model API endpoints with a traditional synchronous Python for loop. For this experiment, we are going to compare the time it takes for `prompto` to obtain 100 responses from a model API endpoint and the time it takes for a synchronous Python for loop to obtain the same 100 responses.\n",
    "\n",
    "We choose three API endpoints for this experiment:\n",
    "- [OpenAI API](../../docs/openai.md)\n",
    "- [Gemini API](../../docs/gemini.md)\n",
    "- [Ollama API](../../docs/ollama.md) (which is locally hosted)\n",
    "\n",
    "For this experiment, we will need to set up the following environment variables:\n",
    "- `OPENAI_API_KEY`: the API key for the OpenAI API\n",
    "- `GEMINI_API_KEY`: the API key for the Gemini API\n",
    "- `OLLAMA_API_ENDPOINT`: the endpoint for the Ollama API\n",
    "\n",
    "To set these environment variables, one can simply have these in a `.env` file which specifies these environment variables as key-value pairs:\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR-OPENAI=KEY>\n",
    "GEMINI_API_KEY=<YOUR-GEMINI-KEY>\n",
    "OLLAMA_API_ENDPOINT=<YOUR-OLLAMA-ENDPOINT>\n",
    "```\n",
    "\n",
    "If you make this file, you can run the following which should return True if it's found one, or False otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment, we take a sample of 100 prompts from the [`alpaca_data.json`](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json) from the [`tatsu-lab/stanford_alpaca` Github repo](https://github.com/tatsu-lab/stanford_alpaca) and using the prompt template provided by the authors of the repo. To see how we obtain the prompts, please refer to the [alpaca_sample_generation.ipynb](./alpaca_sample_generation.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompts = load_prompts(\"./sample_prompts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create our experiment files using the `generate_experiment_1_file` function in the `dataset_utils.py` file in this directory. This function will just take these prompts and create a jsonl file with the prompts in the format that `prompto` expects. We will save these input files into `./data/input` and use `./data` are our pipeline data folder.\n",
    "\n",
    "See the [pipeline data docs](../../docs/pipeline.md) for more information about the pipeline data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_EXPERIMENT_FILE = \"./data/input/openai.jsonl\"\n",
    "GEMINI_EXPERIMENT_FILE = \"./data/input/gemini.jsonl\"\n",
    "OLLAMA_EXPERIMENT_FILE = \"./data/input/ollama.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we query the following models:\n",
    "- `gpt-3.5-turbo` for the OpenAI API\n",
    "- `gemini-1.5-flash` for the Gemini API\n",
    "- `llama3` (8B, 4bit quantised) for the Ollama API\n",
    "\n",
    "Notice that each different API has different argument names for the generation configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=OPENAI_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"openai\",\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    params={\"n\": 1, \"temperature\": 0.9, \"max_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=GEMINI_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"gemini\",\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    params={\"candidate_count\": 1, \"temperature\": 0.9, \"max_output_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_experiment_1_file(\n",
    "    path=OLLAMA_EXPERIMENT_FILE,\n",
    "    prompts=alpaca_prompts,\n",
    "    api=\"ollama\",\n",
    "    model_name=\"llama3\",\n",
    "    params={\"temperature\": 0.9, \"num_predict\": 100, \"seed\": 42},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each API, we will compare runtimes for using `prompto` and a synchronous Python for loop to obtain 100 responses from the API.\n",
    "\n",
    "We use the `send_prompts_sync` function from the `api_utils.py` file in the directory for the synchronous Python for loop approach. We can run experiments using the `prompto.experiment.Experiment.process` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_times = {}\n",
    "prompto_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE)): 100\n",
      "len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE)): 100\n",
      "len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE)): 100\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE)): {len(load_prompt_dicts(OPENAI_EXPERIMENT_FILE))}\"\n",
    ")\n",
    "print(\n",
    "    f\"len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE)): {len(load_prompt_dicts(GEMINI_EXPERIMENT_FILE))}\"\n",
    ")\n",
    "print(\n",
    "    f\"len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE)): {len(load_prompt_dicts(OLLAMA_EXPERIMENT_FILE))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `prompto` via the command line\n",
    "\n",
    "We can also run the experiments via the command line. The command is as follows (assuming that your working directory is the current directory of this notebook, i.e. `examples/system-demo`):\n",
    "```bash\n",
    "prompto_run_experiment --file data/input/openai.jsonl --max_queries 500\n",
    "prompto_run_experiment --file data/input/gemini.jsonl --max_queries 500\n",
    "prompto_run_experiment --file data/input/ollam,a.jsonl --max_queries 40\n",
    "```\n",
    "\n",
    "But for this notebook, we will time the experiments and save them to the `sync_times` and `prompto_times` dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "openai_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(OPENAI_EXPERIMENT_FILE))\n",
    "sync_times[\"openai\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running `prompto` with the OpenAI API, we can run prompts at 500QPM. It is possible to have tiers which offer a higher rate limit, but we will use the 500QPM rate limit for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 500 QPM with RI of 0.12s  (attempt 1/3): 100%|██████████| 100/100 [00:12<00:00,  8.20query/s]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 100/100 [00:01<00:00, 58.66query/s]\n"
     ]
    }
   ],
   "source": [
    "openai_experiment = Experiment(\n",
    "    file_name=\"openai.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=500)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "openai_responses, _ = await openai_experiment.process()\n",
    "prompto_times[\"openai\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126.30979299545288, 13.91887378692627)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"openai\"], prompto_times[\"openai\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:43<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gemini_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(GEMINI_EXPERIMENT_FILE))\n",
    "sync_times[\"gemini\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the OpenAI API, for running `prompto` with the Gemini API, we can run prompts at 500QPM. It is possible to have tiers which offer a higher rate limit, but we will use the 500QPM rate limit for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 500 QPM with RI of 0.12s  (attempt 1/3):   0%|          | 0/100 [00:00<?, ?query/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 500 QPM with RI of 0.12s  (attempt 1/3): 100%|██████████| 100/100 [00:12<00:00,  8.17query/s]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 100/100 [00:01<00:00, 55.77query/s]\n"
     ]
    }
   ],
   "source": [
    "gemini_experiment = Experiment(\n",
    "    file_name=\"gemini.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=500)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "gemini_responses, _ = await gemini_experiment.process()\n",
    "prompto_times[\"gemini\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163.48729801177979, 14.094270944595337)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"gemini\"], prompto_times[\"gemini\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Before running the Ollama experiment, we will just send an empty prompt request with the `llama3` model to 1) check that the model is available and working, and 2) to ensure that the model is loaded in memory - sending an empty request in Ollama ensures pre-loading of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    f\"{os.environ.get('OLLAMA_API_ENDPOINT')}/api/generate\", json={\"model\": \"llama3\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:31<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ollama_sync = send_prompts_sync(prompt_dicts=load_prompt_dicts(OLLAMA_EXPERIMENT_FILE))\n",
    "sync_times[\"ollama\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ollama, we use a locally hosted API endpoint. While the Ollama API implements a queue system to allow for asynchronous requests, it actually still only processes one request at a time so we expect a modest speedup when using `prompto` to query the Ollama API. We will use a 40QPM rate limit for this experiment as we are using a M1 Pro Macbook 14\" model to run Ollama for this experiment which cannot handle much higher than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 100 queries at 40 QPM with RI of 1.5s  (attempt 1/3): 100%|██████████| 100/100 [02:30<00:00,  1.50s/query]\n",
      "Waiting for responses  (attempt 1/3): 100%|██████████| 100/100 [01:58<00:00,  1.18s/query]\n"
     ]
    }
   ],
   "source": [
    "ollama_experiment = Experiment(\n",
    "    file_name=\"ollama.jsonl\", settings=Settings(data_folder=\"./data\", max_queries=40)\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "ollama_responses, _ = await ollama_experiment.process()\n",
    "prompto_times[\"ollama\"] = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271.4494206905365, 268.59372997283936)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times[\"ollama\"], prompto_times[\"ollama\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Here, we report the final runtimes for each API and the difference in time between the `prompto` and synchronous Python for loop approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': 126.30979299545288,\n",
       " 'gemini': 163.48729801177979,\n",
       " 'ollama': 271.4494206905365}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': 13.91887378692627,\n",
       " 'gemini': 14.094270944595337,\n",
       " 'ollama': 268.59372997283936}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompto_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
