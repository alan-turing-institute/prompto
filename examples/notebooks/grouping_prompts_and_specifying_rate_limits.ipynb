{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping prompts and specifying rate limits\n",
    "\n",
    "When running the pipeline or an experiment, there are certain settings to define how to run the experiments which are described in the [pipeline documentation](https://alan-turing-institute.github.io/prompto/docs/pipeline/#pipeline-settings). In the [Specifying rate limits documentation](https://alan-turing-institute.github.io/prompto/docs/rate_limits/), we have seen how we can specify rate limits for the pipeline in the command line interfaces for running the pipeline with [`prompto_run_pipeline`](https://alan-turing-institute.github.io/prompto/docs/commands/#running-the-pipeline) and for running a particular experiment file with [`prompto_run_experiment`](https://alan-turing-institute.github.io/prompto/docs/commands/#running-an-experiment-file). In this notebook, we will walkthrough the examples in the documentation to see how we can specify rate limits for the pipeline and for experiments.\n",
    "\n",
    "We will consider three examples of experiment files which are found in the [input folder of the `parallel_data_example`](https://github.com/alan-turing-institute/prompto/tree/main/examples/notebooks/parallel_data_example/input) directory. The experiment files are:\n",
    "1. [documentation_example.jsonl](https://github.com/alan-turing-institute/prompto/blob/main/examples/notebooks/parallel_data_example/input/documentation_example.jsonl)\n",
    "2. [documentation_example_groups_1.jsonl](https://github.com/alan-turing-institute/prompto/blob/main/examples/notebooks/parallel_data_example/input/documentation_example_groups_1.jsonl)\n",
    "3. [documentation_example_groups_2.jsonl](https://github.com/alan-turing-institute/prompto/blob/main/examples/notebooks/parallel_data_example/input/documentation_example_groups_2.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompto import Settings, Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"parallel_data_example\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parallel processing\n",
    "\n",
    "As noted in the [Specifying rate limits documentation](https://alan-turing-institute.github.io/prompto/docs/rate_limits/), parallel processing of the prompts (meaning that different groups of prompts are processed and sent to APIs in parallel) should be enabled in most settings where the experiment file includes more than one model. Typically this is even true in cases where we are querying the same API type but for different models since the rate limits are usually per model.\n",
    "\n",
    "To use parallel processing of the prompts, we need to first split the prompts into different queues (or groups). For most settings, splitting by API and certain models is sufficient. However, in some cases, we may want to split the prompts into different groups manually and this can be done by using the `\"group\"` key in the experiment file.\n",
    "\n",
    "When we obtain the groups of prompts for parallel processing, what really is happening in the code (see source code for the `prompto.experiment_processing.Experiment.group_prompts` method) is that we loop over the prompts in the experiment file and assign them to different queues/groups based on:\n",
    "1. the `\"group\"` key if it is present in the prompt dictionary\n",
    "2. the `\"api\"` key\n",
    "\n",
    "Since we also allow for splitting according to the `\"model_name\"` key, we also look at if a rate limit has been specified for a particular model within the group or API. We specify rate limits via the `--max-queries-json` or `-mqj` flag in the [commands line interfaces](https://alan-turing-institute.github.io/prompto/docs/commands/) or via the `max_queries_dict` argument in the `Settings` object in the `prompto` library.\n",
    "\n",
    "In the examples below, we will see how we can specify rate limits for different APIs and models in the experiment files. We will also see how we can use the `\"group\"` key in the experiment file to group prompts manually and specify rate limits for each group.\n",
    "\n",
    "## Examples\n",
    "\n",
    "First, we will look at the [documentation_example.jsonl](https://github.com/alan-turing-institute/prompto/blob/main/examples/notebooks/parallel_data_example/input/documentation_example.jsonl) experiment file which has prompts for three different APIs (`gemini`, `openai` and `ollama`) for 6 different models (`gemini-1.0-pro`, `gemini-1.5-pro`, `gpt3.5-turbo`, `gpt4`, `llama3` and `mistral`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 0, \"api\": \"gemini\", \"model_name\": \"gemini-1.0-pro\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 1, \"api\": \"gemini\", \"model_name\": \"gemini-1.0-pro\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 2, \"api\": \"gemini\", \"model_name\": \"gemini-1.5-pro\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 3, \"api\": \"gemini\", \"model_name\": \"gemini-1.5-pro\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 4, \"api\": \"openai\", \"model_name\": \"gpt3.5-turbo\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 5, \"api\": \"openai\", \"model_name\": \"gpt3.5-turbo\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 6, \"api\": \"openai\", \"model_name\": \"gpt4\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 7, \"api\": \"openai\", \"model_name\": \"gpt4\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 8, \"api\": \"ollama\", \"model_name\": \"llama3\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 9, \"api\": \"ollama\", \"model_name\": \"llama3\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 10, \"api\": \"ollama\", \"model_name\": \"mistral\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 11, \"api\": \"ollama\", \"model_name\": \"mistral\", \"prompt\": \"What is the capital of Germany?\"}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{data_folder}/input/documentation_example.jsonl\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same rate limit for all APIs\n",
    "\n",
    "Recall for each `Experiment`, we need to pass in the path to a jsonl file and a `Settings` object which stores paths to relevant data folders and also some parameter settings for how to run the particular experiment. For an overview of the `Settings` and `Experiment` classes see the [Running experiments notebook](https://alan-turing-institute.github.io/prompto/examples/notebooks/running_experiments/).\n",
    "\n",
    "By default, the `Settings` object has the `parallel` attribute set to `False`. Recall we can simply print the settings object to see the current settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=False\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "settings = Settings(data_folder=data_folder, max_queries=5)\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply initialise an `Experiment` object for this experiment and the prompts in that experiment are stored in the `experiment_prompts` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'api': 'gemini',\n",
       "  'model_name': 'gemini-1.0-pro',\n",
       "  'prompt': 'What is the capital of France?'},\n",
       " {'id': 1,\n",
       "  'api': 'gemini',\n",
       "  'model_name': 'gemini-1.0-pro',\n",
       "  'prompt': 'What is the capital of Germany?'},\n",
       " {'id': 2,\n",
       "  'api': 'gemini',\n",
       "  'model_name': 'gemini-1.5-pro',\n",
       "  'prompt': 'What is the capital of France?'},\n",
       " {'id': 3,\n",
       "  'api': 'gemini',\n",
       "  'model_name': 'gemini-1.5-pro',\n",
       "  'prompt': 'What is the capital of Germany?'},\n",
       " {'id': 4,\n",
       "  'api': 'openai',\n",
       "  'model_name': 'gpt3.5-turbo',\n",
       "  'prompt': 'What is the capital of France?'},\n",
       " {'id': 5,\n",
       "  'api': 'openai',\n",
       "  'model_name': 'gpt3.5-turbo',\n",
       "  'prompt': 'What is the capital of Germany?'},\n",
       " {'id': 6,\n",
       "  'api': 'openai',\n",
       "  'model_name': 'gpt4',\n",
       "  'prompt': 'What is the capital of France?'},\n",
       " {'id': 7,\n",
       "  'api': 'openai',\n",
       "  'model_name': 'gpt4',\n",
       "  'prompt': 'What is the capital of Germany?'},\n",
       " {'id': 8,\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3',\n",
       "  'prompt': 'What is the capital of France?'},\n",
       " {'id': 9,\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'llama3',\n",
       "  'prompt': 'What is the capital of Germany?'},\n",
       " {'id': 10,\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'mistral',\n",
       "  'prompt': 'What is the capital of France?'},\n",
       " {'id': 11,\n",
       "  'api': 'ollama',\n",
       "  'model_name': 'mistral',\n",
       "  'prompt': 'What is the capital of Germany?'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(file_name=\"documentation_example.jsonl\", settings=settings)\n",
    "experiment.experiment_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `experiment_prompts` attribute is read only (which is implemented using a `@property` decorator) and so we cannot change the prompts directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot set the experiment_prompts attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment_prompts\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msomething else\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/prompto/src/prompto/experiment_processing.py:115\u001b[0m, in \u001b[0;36mExperiment.experiment_prompts\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@experiment_prompts\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexperiment_prompts\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set the experiment_prompts attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot set the experiment_prompts attribute"
     ]
    }
   ],
   "source": [
    "experiment.experiment_prompts = \"something else\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment file, we notice that we have three `\"api\"` keys present (`gemini`, `openai` and `ollama`). If no `max_queries_dict` is passed into the `Settings` object, then as we noted above, the prompts will be grouped first by their `\"group\"` key and then by their `\"api\"` key. In this example, we have no prompts with a `\"group\"` key and so the prompts will be grouped by their `\"api\"` key.\n",
    "\n",
    "The groups of prompts are stored in the `grouped_experiment_prompts` attribute which again is a read only attribute and it's only initialised when we try to access the attribute. We can see by default the underlying attribute `_grouped_experiment_prompts` is an empty dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment._grouped_experiment_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we access the `grouped_experiment_prompts` attribute, the prompts are grouped by their `\"api\"` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The 'parallel' attribute in the Settings object is set to False, so grouping will not be used when processing the experiment prompts. Set 'parallel' to True to use grouping and parallel processing of prompts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gemini': {'prompt_dicts': [{'id': 0,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 1,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 2,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 3,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5},\n",
       " 'openai': {'prompt_dicts': [{'id': 4,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 5,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 6,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 7,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5},\n",
       " 'ollama': {'prompt_dicts': [{'id': 8,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 9,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 10,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 11,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the warning message that is logged when accessing this attribute. We got this message since the `parallel` attribute in the `Settings` object is set to `False`. We still get the groups of prompts but if we run the experiment, the prompts will not be processed in these different groups/queues in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The 'parallel' attribute in the Settings object is set to False, so grouping will not be used when processing the experiment prompts. Set 'parallel' to True to use grouping and parallel processing of prompts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts == experiment._grouped_experiment_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that `_grouped_experiment_prompts` is now a dictionary with keys as the different APIs and the values are dictionaries with keys `\"prompt_dicts\"` which is a list of the prompts for that API and `\"rate_limit\"` which is the rate limit for that API. We can see that the rate limit here for each API is set to the default rate limit of `5` which is given by the `Settings` object for the experiment and which we set above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The 'parallel' attribute in the Settings object is set to False, so grouping will not be used when processing the experiment prompts. Set 'parallel' to True to use grouping and parallel processing of prompts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['gemini', 'openai', 'ollama'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.settings.max_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, this attribute is read only so we will not be able to change the groups directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot set the grouped_experiment_prompts attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_experiment_prompts\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msomething else\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/prompto/src/prompto/experiment_processing.py:136\u001b[0m, in \u001b[0;36mExperiment.grouped_experiment_prompts\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;129m@grouped_experiment_prompts\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrouped_experiment_prompts\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set the grouped_experiment_prompts attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot set the grouped_experiment_prompts attribute"
     ]
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts = \"something else\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful method in the `Experiment` class is the `grouped_experiment_prompts_summary` method which returns a dictionary where the keys are the API names and the values are a string which summarises the number of prompts for that API and the rate limit for that API. This is useful to see a summary of the groups of prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The 'parallel' attribute in the Settings object is set to False, so grouping will not be used when processing the experiment prompts. Set 'parallel' to True to use grouping and parallel processing of prompts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gemini': '4 queries at 5 queries per minute',\n",
       " 'openai': '4 queries at 5 queries per minute',\n",
       " 'ollama': '4 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different rate limits for each API type\n",
    "\n",
    "To build on the above example, we can set different rate limits for each API type by passing in a dictionary which specifies the rate limits for each API type. We can do this by passing in a `max_queries_dict` argument to the `Settings` object (or passing a json to the `--max-queries-json` or `-mqj` flag in the [commands line interfaces](https://alan-turing-institute.github.io/prompto/docs/commands/)) where the keys are the API names and the values are the rate limits for that API. We can see how this is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:max_queries_dict is provided and not empty, but parallel is set to False, so max_queries_dict will not be used. Set parallel to True to use max_queries_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=False\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\"openai\": 20, \"gemini\": 10}\n",
    "settings = Settings(\n",
    "    data_folder=data_folder,\n",
    "    max_queries=5,\n",
    "    max_queries_dict=max_queries_dict,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the warning provided here! We passed in a `max_queries_dict` to the `Settings` object but the `parallel` attribute is still set to `False`. We can remove this warning by setting the `parallel` attribute to `True` as the warning suggests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=True, max_queries_dict={'openai': 20, 'gemini': 10}\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\"openai\": 20, \"gemini\": 10}\n",
    "settings = Settings(\n",
    "    data_folder=data_folder,\n",
    "    max_queries=5,\n",
    "    max_queries_dict=max_queries_dict,\n",
    "    parallel=True,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice above how the `max_queries_dict` is only printed when the `parallel` attribute is set to `True`.\n",
    "\n",
    "Let's now see how the prompts are grouped and what the `grouped_experiment_prompts` attribute looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': {'prompt_dicts': [{'id': 4,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 5,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 6,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 7,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 20},\n",
       " 'gemini': {'prompt_dicts': [{'id': 0,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 1,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 2,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 3,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 10},\n",
       " 'ollama': {'prompt_dicts': [{'id': 8,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 9,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 10,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 11,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(file_name=\"documentation_example.jsonl\", settings=settings)\n",
    "experiment.grouped_experiment_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['openai', 'gemini', 'ollama'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': '4 queries at 20 queries per minute',\n",
       " 'gemini': '4 queries at 10 queries per minute',\n",
       " 'ollama': '4 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that we have the same grouping (by API type) as above, but the rate limits for `gemini` and `openai` have been set to `20` and `10` respectively. When processing the experiment, we will send the \"gemini\" prompts at a rate of 20 prompts per minute and the \"openai\" prompts at a rate of 10 prompts per minute. We have not specified the `ollama` rate limit and so it will be set to the default rate limit of `5` which was passed into the `Settings` object via the `max_queries` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different rate limits for each API type and model\n",
    "\n",
    "For this example, we have different models within each API. For `gemini`, we have `\"gemini-1.0-pro\"` and `\"gemini-1.5-pro\"`, for `openai`, we have `\"gpt3.5-turbo\"` and `\"gpt4\"` and for `ollama`, we have `\"llaam3\"` and `\"mistral\"`. \n",
    "\n",
    "To specify model-specific rate limits, instead of passing in an integer value for an API type like above, we can actually pass in another dictionary where the keys are model names and the values are the rate limits for that model, i.e. `max_queries_dict` can be a nested dictionary. Note that we do not need to specify rates for every model but only for the models we want to specify rates for. Everything else will be set to the default rate limit.\n",
    "\n",
    "We can see how this is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=True, max_queries_dict={'gemini': {'gemini-1.5-pro': 20}, 'openai': {'gpt4': 10, 'gpt3.5-turbo': 20}}\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\n",
    "    \"gemini\": {\"gemini-1.5-pro\": 20},\n",
    "    \"openai\": {\"gpt4\": 10, \"gpt3.5-turbo\": 20},\n",
    "}\n",
    "settings = Settings(\n",
    "    data_folder=data_folder,\n",
    "    max_queries=5,\n",
    "    max_queries_dict=max_queries_dict,\n",
    "    parallel=True,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are specifying that `\"gemini-1.5-pro\"` from the `gemini` API should have a rate limit of `20`, the `\"gpt4\"` and `\"gpt3.5-turbo\"` models from the `openai` API should have rate limits of `10` and `20` respectively. Everything else will be set to the default rate limit of `5`.\n",
    "\n",
    "Let's now see how the prompts are grouped and what the `grouped_experiment_prompts` attribute looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini-gemini-1.5-pro': {'prompt_dicts': [{'id': 2,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 3,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 20},\n",
       " 'openai-gpt4': {'prompt_dicts': [{'id': 6,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 7,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 10},\n",
       " 'openai-gpt3.5-turbo': {'prompt_dicts': [{'id': 4,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 5,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 20},\n",
       " 'gemini': {'prompt_dicts': [{'id': 0,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 1,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5},\n",
       " 'openai': {'prompt_dicts': [], 'rate_limit': 5},\n",
       " 'ollama': {'prompt_dicts': [{'id': 8,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 9,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 10,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 11,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(file_name=\"documentation_example.jsonl\", settings=settings)\n",
    "experiment.grouped_experiment_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini-gemini-1.5-pro': '2 queries at 20 queries per minute',\n",
       " 'openai-gpt4': '2 queries at 10 queries per minute',\n",
       " 'openai-gpt3.5-turbo': '2 queries at 20 queries per minute',\n",
       " 'gemini': '2 queries at 5 queries per minute',\n",
       " 'openai': '0 queries at 5 queries per minute',\n",
       " 'ollama': '4 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, when we group the prompts, we are actually just looping over the prompts in the experiment and looking at the `\"api\"` key (if the `\"group\"` key is not present). We then look at the `\"model_name\"` key if it is present and if a rate limit has been specified for that model, we add it to a model-specific group. If no rate limit has been specified for that model, we add it to the default group for that API.\n",
    "\n",
    "For `gemini`, we can see that we have a model-specific group for `\"gemini-1.5-pro\"` called `\"gemini-gemini-1.5-pro\"` and we have two queries for that model and this has a rate limit of 20 queries per minute as specified by the `max_queries_dict` above. We also have a default group for `gemini` called `\"gemini\"` which catches all other `gemini` prompts. We did not specify any default rate limit for `gemini` and so it will be set to the default rate limit of `5`.\n",
    "\n",
    "For `openai`, we can see that we have two model-specific groups for `\"gpt4\"` and `\"gpt3.5-turbo\"` called `\"openai-gpt4\"` and `\"openai-gpt3.5-turbo\"` respectively which have the correct rate limits as specified by the `max_queries_dict` above. We also have a default group for `openai` called `\"openai\"` which catches all other `openai` prompts. For this experiment file, there are no other `openai` prompts and so this group is empty.\n",
    "\n",
    "Finally, we still have the group of `ollama` prompts which is called `\"ollama\"` and this has the default rate limit of `5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default rate limits for APIs\n",
    "\n",
    "If we want to specify the default rate limit for a given API type, we can do this by specifying a rate limit for `\"default\"` in the `max_queries_dict`. This will set the default rate limit for the API which will include all prompts that do not have a model-specific rate limit. We can see how this is done below:\n",
    "\n",
    "Note for specifying the `ollama` API, writing `\"ollama\": 4` is equivalent to writing `\"ollama\": {\"default\": 4}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=True, max_queries_dict={'gemini': {'default': 30, 'gemini-1.5-pro': 20}, 'openai': {'gpt4': 10, 'gpt3.5-turbo': 20}, 'ollama': 4}\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\n",
    "    \"gemini\": {\"default\": 30, \"gemini-1.5-pro\": 20},\n",
    "    \"openai\": {\"gpt4\": 10, \"gpt3.5-turbo\": 20},\n",
    "    \"ollama\": 4,\n",
    "}\n",
    "settings = Settings(\n",
    "    data_folder=data_folder,\n",
    "    max_queries=5,\n",
    "    max_queries_dict=max_queries_dict,\n",
    "    parallel=True,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the rate limits for `gemini` and `ollama` have been specified as `20` and `4` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini': '2 queries at 30 queries per minute',\n",
       " 'gemini-gemini-1.5-pro': '2 queries at 20 queries per minute',\n",
       " 'openai-gpt4': '2 queries at 10 queries per minute',\n",
       " 'openai-gpt3.5-turbo': '2 queries at 20 queries per minute',\n",
       " 'ollama': '4 queries at 4 queries per minute',\n",
       " 'openai': '0 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(file_name=\"documentation_example.jsonl\", settings=settings)\n",
    "experiment.grouped_experiment_prompts_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying models or APIs that don't exist in the experiment file\n",
    "\n",
    "Note that if you specify a API/group or model that does not exist in the experiment file, there will be a group/queue created for that API/group or model but it will be empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=True, max_queries_dict={'gemini': {'default': 30, 'gemini-1.5-pro': 20}, 'openai': {'gpt4': 10, 'gpt3.5-turbo': 20}, 'ollama': {'llama3': 3, 'mistral': 3, 'unknown-model': 4}, 'unknown-group-or-api': 25}\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\n",
    "    \"gemini\": {\"default\": 30, \"gemini-1.5-pro\": 20},\n",
    "    \"openai\": {\"gpt4\": 10, \"gpt3.5-turbo\": 20},\n",
    "    \"ollama\": {\n",
    "        \"llama3\": 3,\n",
    "        \"mistral\": 3,\n",
    "        \"unknown-model\": 4,\n",
    "    },\n",
    "    \"unknown-group-or-api\": 25,\n",
    "}\n",
    "settings = Settings(\n",
    "    data_folder=data_folder,\n",
    "    max_queries=5,\n",
    "    max_queries_dict=max_queries_dict,\n",
    "    parallel=True,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini': '2 queries at 30 queries per minute',\n",
       " 'gemini-gemini-1.5-pro': '2 queries at 20 queries per minute',\n",
       " 'openai-gpt4': '2 queries at 10 queries per minute',\n",
       " 'openai-gpt3.5-turbo': '2 queries at 20 queries per minute',\n",
       " 'ollama-llama3': '2 queries at 3 queries per minute',\n",
       " 'ollama-mistral': '2 queries at 3 queries per minute',\n",
       " 'ollama-unknown-model': '0 queries at 4 queries per minute',\n",
       " 'unknown-group-or-api': '0 queries at 25 queries per minute',\n",
       " 'openai': '0 queries at 5 queries per minute',\n",
       " 'ollama': '0 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(file_name=\"documentation_example.jsonl\", settings=settings)\n",
    "experiment.grouped_experiment_prompts_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full control: Using the \"groups\" key to define user-specified groups of prompts\n",
    "\n",
    "In some cases, we may want to group prompts manually. This can be done by using the `\"group\"` key in the experiment file. We now look at the [documentation_example_groups_1.jsonl](https://github.com/alan-turing-institute/prompto/blob/main/examples/notebooks/parallel_data_example/input/documentation_example_groups_1.jsonl) experiment file which has prompts for three different APIs (`gemini`, `openai` and `ollama`) for 6 different models (`gemini-1.0-pro`, `gemini-1.5-pro`, `gpt3.5-turbo`, `gpt4`, `llaam3` and `mistral`). We have manually grouped the prompts into three groups: `\"group1\"`, `\"group2\"` and `\"group3\"`.\n",
    "\n",
    "Note that when specifying the `\"group\"` key, the prompts will be grouped by this key and not by the `\"api\"` key. We can see how the prompts are grouped below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 0, \"api\": \"gemini\", \"model_name\": \"gemini-1.0-pro\", \"prompt\": \"What is the capital of France?\", \"group\": \"group1\"}\n",
      "{\"id\": 1, \"api\": \"gemini\", \"model_name\": \"gemini-1.0-pro\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group2\"}\n",
      "{\"id\": 2, \"api\": \"gemini\", \"model_name\": \"gemini-1.5-pro\", \"prompt\": \"What is the capital of France?\", \"group\": \"group1\"}\n",
      "{\"id\": 3, \"api\": \"gemini\", \"model_name\": \"gemini-1.5-pro\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group2\"}\n",
      "{\"id\": 4, \"api\": \"openai\", \"model_name\": \"gpt3.5-turbo\", \"prompt\": \"What is the capital of France?\", \"group\": \"group1\"}\n",
      "{\"id\": 5, \"api\": \"openai\", \"model_name\": \"gpt3.5-turbo\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group2\"}\n",
      "{\"id\": 6, \"api\": \"openai\", \"model_name\": \"gpt4\", \"prompt\": \"What is the capital of France?\", \"group\": \"group1\"}\n",
      "{\"id\": 7, \"api\": \"openai\", \"model_name\": \"gpt4\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group2\"}\n",
      "{\"id\": 8, \"api\": \"ollama\", \"model_name\": \"llama3\", \"prompt\": \"What is the capital of France?\", \"group\": \"group3\"}\n",
      "{\"id\": 9, \"api\": \"ollama\", \"model_name\": \"llama3\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group3\"}\n",
      "{\"id\": 10, \"api\": \"ollama\", \"model_name\": \"mistral\", \"prompt\": \"What is the capital of France?\", \"group\": \"group3\"}\n",
      "{\"id\": 11, \"api\": \"ollama\", \"model_name\": \"mistral\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group3\"}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{data_folder}/input/documentation_example_groups_1.jsonl\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting rate limits for groups works in the exact same way as setting rate limits for APIs. We simply pass in a dictionary where the keys are the group names and the values are the rate limits for that group. We can see how this is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=True, max_queries_dict={}\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\"group1\": 5, \"group2\": 10, \"group3\": 15}\n",
    "settings = Settings(data_folder=data_folder, max_queries=5, parallel=True)\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group1': {'prompt_dicts': [{'id': 0,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group1'},\n",
       "   {'id': 2,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group1'},\n",
       "   {'id': 4,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group1'},\n",
       "   {'id': 6,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group1'}],\n",
       "  'rate_limit': 5},\n",
       " 'group2': {'prompt_dicts': [{'id': 1,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group2'},\n",
       "   {'id': 3,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group2'},\n",
       "   {'id': 5,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group2'},\n",
       "   {'id': 7,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group2'}],\n",
       "  'rate_limit': 5},\n",
       " 'group3': {'prompt_dicts': [{'id': 8,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group3'},\n",
       "   {'id': 9,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group3'},\n",
       "   {'id': 10,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group3'},\n",
       "   {'id': 11,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group3'}],\n",
       "  'rate_limit': 5}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "    file_name=\"documentation_example_groups_1.jsonl\", settings=settings\n",
    ")\n",
    "experiment.grouped_experiment_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group1': '4 queries at 5 queries per minute',\n",
       " 'group2': '4 queries at 5 queries per minute',\n",
       " 'group3': '4 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing using the \"api\" and \"group\" keys to define groups\n",
    "\n",
    "\n",
    "It is possible to have an experiment file where only some of the prompts have a `\"group\"` key. We consider one here in the [documentation_example_groups_2.jsonl](https://github.com/alan-turing-institute/prompto/blob/main/examples/notebooks/parallel_data_example/input/documentation_example_groups_2.jsonl) experiment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 0, \"api\": \"gemini\", \"model_name\": \"gemini-1.0-pro\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 1, \"api\": \"gemini\", \"model_name\": \"gemini-1.0-pro\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 2, \"api\": \"gemini\", \"model_name\": \"gemini-1.5-pro\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 3, \"api\": \"gemini\", \"model_name\": \"gemini-1.5-pro\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 4, \"api\": \"openai\", \"model_name\": \"gpt3.5-turbo\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 5, \"api\": \"openai\", \"model_name\": \"gpt3.5-turbo\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 6, \"api\": \"openai\", \"model_name\": \"gpt4\", \"prompt\": \"What is the capital of France?\"}\n",
      "{\"id\": 7, \"api\": \"openai\", \"model_name\": \"gpt4\", \"prompt\": \"What is the capital of Germany?\"}\n",
      "{\"id\": 8, \"api\": \"ollama\", \"model_name\": \"llama3\", \"prompt\": \"What is the capital of France?\", \"group\": \"group1\"}\n",
      "{\"id\": 9, \"api\": \"ollama\", \"model_name\": \"llama3\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group1\"}\n",
      "{\"id\": 10, \"api\": \"ollama\", \"model_name\": \"mistral\", \"prompt\": \"What is the capital of France?\", \"group\": \"group1\"}\n",
      "{\"id\": 11, \"api\": \"ollama\", \"model_name\": \"mistral\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group1\"}\n",
      "{\"id\": 12, \"api\": \"ollama\", \"model_name\": \"gemma\", \"prompt\": \"What is the capital of France?\", \"group\": \"group2\"}\n",
      "{\"id\": 13, \"api\": \"ollama\", \"model_name\": \"gemma\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group2\"}\n",
      "{\"id\": 14, \"api\": \"ollama\", \"model_name\": \"phi3\", \"prompt\": \"What is the capital of France?\", \"group\": \"group2\"}\n",
      "{\"id\": 15, \"api\": \"ollama\", \"model_name\": \"phi3\", \"prompt\": \"What is the capital of Germany?\", \"group\": \"group2\"}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{data_folder}/input/documentation_example_groups_2.jsonl\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, we first try to place prompts into the right groups based on the `\"group\"` key and then based on the `\"api\"` key. We will specify rate limits for two groups here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=True, max_queries_dict={'group1': 5, 'group2': 10}\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\n",
    "    \"group1\": 5,\n",
    "    \"group2\": 10,\n",
    "}\n",
    "settings = Settings(\n",
    "    data_folder=data_folder,\n",
    "    max_queries=5,\n",
    "    max_queries_dict=max_queries_dict,\n",
    "    parallel=True,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the prompts with \"group\" keys are placed within their respective groups and the remaining prompts are grouped by their \"api\" key (either `gemini` or `openai` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group1': {'prompt_dicts': [{'id': 8,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group1'},\n",
       "   {'id': 9,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'llama3',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group1'},\n",
       "   {'id': 10,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group1'},\n",
       "   {'id': 11,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'mistral',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group1'}],\n",
       "  'rate_limit': 5},\n",
       " 'group2': {'prompt_dicts': [{'id': 12,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'gemma',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group2'},\n",
       "   {'id': 13,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'gemma',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group2'},\n",
       "   {'id': 14,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'phi3',\n",
       "    'prompt': 'What is the capital of France?',\n",
       "    'group': 'group2'},\n",
       "   {'id': 15,\n",
       "    'api': 'ollama',\n",
       "    'model_name': 'phi3',\n",
       "    'prompt': 'What is the capital of Germany?',\n",
       "    'group': 'group2'}],\n",
       "  'rate_limit': 10},\n",
       " 'gemini': {'prompt_dicts': [{'id': 0,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 1,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.0-pro',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 2,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 3,\n",
       "    'api': 'gemini',\n",
       "    'model_name': 'gemini-1.5-pro',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5},\n",
       " 'openai': {'prompt_dicts': [{'id': 4,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 5,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt3.5-turbo',\n",
       "    'prompt': 'What is the capital of Germany?'},\n",
       "   {'id': 6,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of France?'},\n",
       "   {'id': 7,\n",
       "    'api': 'openai',\n",
       "    'model_name': 'gpt4',\n",
       "    'prompt': 'What is the capital of Germany?'}],\n",
       "  'rate_limit': 5}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "    file_name=\"documentation_example_groups_2.jsonl\", settings=settings\n",
    ")\n",
    "experiment.grouped_experiment_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group1': '4 queries at 5 queries per minute',\n",
       " 'group2': '4 queries at 10 queries per minute',\n",
       " 'gemini': '4 queries at 5 queries per minute',\n",
       " 'openai': '4 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.grouped_experiment_prompts_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-specific rates within groups\n",
    "\n",
    "Specifying model-specific rates within groups works in the exact same way as specifying model-specific rates for APIs. We can see how this is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=parallel_data_example, max_queries=5, max_attempts=3, parallel=True, max_queries_dict={'group1': {'llama3': 10}, 'group2': 10}\n",
      "Subfolders: input_folder=parallel_data_example/input, output_folder=parallel_data_example/output, media_folder=parallel_data_example/media\n"
     ]
    }
   ],
   "source": [
    "max_queries_dict = {\n",
    "    \"group1\": {\"llama3\": 10},\n",
    "    \"group2\": 10,\n",
    "}\n",
    "settings = Settings(\n",
    "    data_folder=data_folder,\n",
    "    max_queries=5,\n",
    "    max_queries_dict=max_queries_dict,\n",
    "    parallel=True,\n",
    ")\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now we see we have split up `group1` further and have a `group1-llama3` grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group1-llama3': '2 queries at 10 queries per minute',\n",
       " 'group2': '4 queries at 10 queries per minute',\n",
       " 'gemini': '4 queries at 5 queries per minute',\n",
       " 'openai': '4 queries at 5 queries per minute',\n",
       " 'group1': '2 queries at 5 queries per minute'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "    file_name=\"documentation_example_groups_2.jsonl\", settings=settings\n",
    ")\n",
    "experiment.grouped_experiment_prompts_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
