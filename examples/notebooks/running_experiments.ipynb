{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing pipeline\n",
    "\n",
    "## Running experiments\n",
    "\n",
    "When running the pipeline, there are a few key classes that we will look at in this notebook:\n",
    "- `Settings`: this defines the settings of the the experiment pipeline which stores the paths to the relevant data folders and the parameters for the pipeline.\n",
    "- `Experiment`: this defines all the variables related to a _single_ experiment. An 'experiment' here is defined by a particular JSONL file which contains the data/prompts for each experiment. Each line in this folder is a particular input to the LLM which we will obtain a response for.\n",
    "- `ExperimentPipeline`: this is the main class for running the full pipeline. The pipeline can be ran using the `ExperimentPipeline.run()` method which will continually check the input folder for new experiments to process.\n",
    "    - This takes in a `Settings` object and for each JSONL file in the input folder, it will create an `Experiment` object and run the experiments sequentially as they are created in the input folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from prompto import Settings, Experiment, ExperimentPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The `Settings` class stores all the relevant information for the pipeline such as:\n",
    "- the paths to the data folders\n",
    "- the (default) maximum number of queries per minute\n",
    "- the number of max retries for failed requests\n",
    "- whether or not to use parallel processing of the prompts\n",
    "- the maximum number of queries per minute for each group of prompts (if parallel processing is enabled) - see the [Grouping prompts and specifying rate limits notebook](grouping_prompts_and_specifying_rate_limits.ipynb) for more information on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"data\" not in os.listdir(\".\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = Settings(data_folder=\"data\", max_queries=50, max_attempts=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the settings object to see the current settings easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=data, max_queries=50, max_attempts=5, parallel=False\n",
      "Subfolders: input_folder=data/input, output_folder=data/output, media_folder=data/media\n"
     ]
    }
   ],
   "source": [
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will just print out the attributes of the settings object to see what is stored in it (although we have just printed these above as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings.data_folder: data\n",
      "settings.input_folder: data/input\n",
      "settings.output_folder: data/output\n",
      "settings.media_folder: data/media\n",
      "settings.max_queries: 50\n",
      "settings.max_attempts: 5\n",
      "settings.parallel: False\n",
      "settings.max_queries_dict: {}\n"
     ]
    }
   ],
   "source": [
    "print(f\"settings.data_folder: {settings.data_folder}\")\n",
    "print(f\"settings.input_folder: {settings.input_folder}\")\n",
    "print(f\"settings.output_folder: {settings.output_folder}\")\n",
    "print(f\"settings.media_folder: {settings.media_folder}\")\n",
    "print(f\"settings.max_queries: {settings.max_queries}\")\n",
    "print(f\"settings.max_attempts: {settings.max_attempts}\")\n",
    "print(f\"settings.parallel: {settings.parallel}\")\n",
    "print(f\"settings.max_queries_dict: {settings.max_queries_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `input_folder`, `output_folder` and `media_folder` attributes are read only (by using the `@property` decorator) and so we cannot change these directly. This is because we want to have consistency with the `data_folder` attribute.\n",
    "\n",
    "So if we try to change the `input_folder, `output_folder` and `media_folder` attributes, it will raise an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "WriteFolderError",
     "evalue": "Cannot set input folder on it's own. Set the 'data_folder' instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWriteFolderError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_folder\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_folder/input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/prompto/src/prompto/settings.py:216\u001b[0m, in \u001b[0;36mSettings.input_folder\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;129m@input_folder\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_folder\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WriteFolderError(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set input folder on it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms own. Set the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_folder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n",
      "\u001b[0;31mWriteFolderError\u001b[0m: Cannot set input folder on it's own. Set the 'data_folder' instead"
     ]
    }
   ],
   "source": [
    "settings.input_folder = \"unknown_folder/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "WriteFolderError",
     "evalue": "Cannot set output folder on it's own. Set the 'data_folder' instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWriteFolderError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_folder\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_folder/output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/prompto/src/prompto/settings.py:228\u001b[0m, in \u001b[0;36mSettings.output_folder\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;129m@output_folder\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutput_folder\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WriteFolderError(\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set output folder on it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms own. Set the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_folder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m     )\n",
      "\u001b[0;31mWriteFolderError\u001b[0m: Cannot set output folder on it's own. Set the 'data_folder' instead"
     ]
    }
   ],
   "source": [
    "settings.output_folder = \"unknown_folder/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "WriteFolderError",
     "evalue": "Cannot set media folder on it's own. Set the 'data_folder' instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWriteFolderError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedia_folder\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_folder/media\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/prompto/src/prompto/settings.py:240\u001b[0m, in \u001b[0;36mSettings.media_folder\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;129m@media_folder\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedia_folder\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WriteFolderError(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set media folder on it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms own. Set the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_folder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n",
      "\u001b[0;31mWriteFolderError\u001b[0m: Cannot set media folder on it's own. Set the 'data_folder' instead"
     ]
    }
   ],
   "source": [
    "settings.media_folder = \"unknown_folder/media\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What really is happening under the hood is we're using a `@property` dectorator and we do not define a setter method for these attributes. This means that we cannot change these attributes directly. Of course, we can change the underlying `_input_folder`, `_output_folder` and `_media_folder` attributes directly if we want to change these, but this is not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings._input_folder = \"unknown_folder/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unknown_folder/input'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.input_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the `data_folder` attribute to a new path if we want to change the data folder. When doing so, it will check if the folder exists, otherwise we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data folder 'unknown_folder' must be a valid path to a folder",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_folder\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/prompto/src/prompto/settings.py:202\u001b[0m, in \u001b[0;36mSettings.data_folder\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;129m@data_folder\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_folder\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# check the data folder exists\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_folder_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# set the data folder\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_folder \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/prompto/src/prompto/settings.py:154\u001b[0m, in \u001b[0;36mSettings.check_folder_exists\u001b[0;34m(data_folder)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# check if data folder exists\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(data_folder):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData folder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a valid path to a folder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Data folder 'unknown_folder' must be a valid path to a folder"
     ]
    }
   ],
   "source": [
    "settings.data_folder = \"unknown_folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the data does exist, it will store the new path and importantly, this will also update the `input_folder`, `output_folder` and `media_folder` attributes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.data_folder = \"data2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `input_folder`, `output_folder` and `media_folder` attributes have been updated to the new corresponding paths.\n",
    "\n",
    "We also create these subfolders if they do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: data_folder=data2, max_queries=50, max_attempts=5, parallel=False\n",
      "Subfolders: input_folder=data2/input, output_folder=data2/output, media_folder=data2/media\n"
     ]
    }
   ],
   "source": [
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings.data_folder: data2\n",
      "settings.input_folder: data2/input\n",
      "settings.output_folder: data2/output\n",
      "settings.media_folder: data2/media\n",
      "settings.max_queries: 50\n",
      "settings.max_attempts: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"settings.data_folder: {settings.data_folder}\")\n",
    "print(f\"settings.input_folder: {settings.input_folder}\")\n",
    "print(f\"settings.output_folder: {settings.output_folder}\")\n",
    "print(f\"settings.media_folder: {settings.media_folder}\")\n",
    "print(f\"settings.max_queries: {settings.max_queries}\")\n",
    "print(f\"settings.max_attempts: {settings.max_attempts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "The `Experiment` class stores all the relevant information for a single experiment. To initialise, we need to pass in the path to the JSONL file which contains the data for the experiment.\n",
    "\n",
    "The `Experiment` class stores several attributes:\n",
    "- `file_name`: the name of the JSONL file\n",
    "- `experiment_name`: the file_name without the `.jsonl` extension\n",
    "- `settings`: `Settings` object which is described above\n",
    "- `output_folder`: the path to the output folder _for the experiment_, e.g. `data_folder/output_folder/experiment_name`\n",
    "- `creation_time`: the time the experiment file was created\n",
    "- `log_file`: the path to the log file for the experiment, e.g. `data_folder/output_folder/experiment_name/{creation_time}_experiment_name.log`\n",
    "- `input_file_path`: the path to the input JSONL file, e.g. `data_folder/input_folder/experiment_name.jsonl`\n",
    "- `output_completed_file_path`: the path to the completed output JSONL file, e.g. `data_folder/output_folder/experiment_name/completed-experiment_name.jsonl`\n",
    "- `output_input_file_path`: the path to the input output JSONL file, e.g. `data_folder/output_folder/experiment_name/input-experiment_name.jsonl` (this is just for logging to know what the input to the experiment was)\n",
    "\n",
    "Essentially, when initialising an `Experiment` object, we construct all the paths that are relevant to that particular experiment such as the log file, the input file path, and the file paths for storing the final output for the experiment. \n",
    "\n",
    "We construct these paths by using the `Settings` object which tells us where all the paths to the relevant folders are.\n",
    "\n",
    "Finally, `Experiment` also stores:\n",
    "- `experiment_prompts` as a list of dictionaries (we just read in the JSONL to get these)\n",
    "- `number_queries`: the number of queries in the experiment (i.e. the length of `experiment_prompts`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\"test.jsonl\", settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test.jsonl'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29-05-2024-17-22-32'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.creation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 9,\n",
       "  'prompt': ['Hello',\n",
       "   \"My name is Bob and I'm 6 years old\",\n",
       "   'How old am I next year?'],\n",
       "  'api': 'unknown-model-name',\n",
       "  'parameters': {'candidate_count': 1,\n",
       "   'max_output_tokens': 64,\n",
       "   'temperature': 1,\n",
       "   'top_k': 40}},\n",
       " {'id': 10,\n",
       "  'prompt': ['Can you give me a random number between 1-10?',\n",
       "   'What is +5 of that number?',\n",
       "   'What is half of that number?'],\n",
       "  'api': 'unknown-model-name',\n",
       "  'parameters': {'candidate_count': 1,\n",
       "   'max_output_tokens': 128,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 40}},\n",
       " {'id': 11,\n",
       "  'prompt': \"How many theaters are there in London's South End?\",\n",
       "  'api': 'unknown-model-name'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.experiment_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.number_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out all the relevant information for the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment.file_name: test.jsonl\n",
      "experiment.input_file_path: data2/input/test.jsonl\n",
      "experiment.output_folder: data2/output/test\n",
      "experiment.output_input_file_out_path: data2/output/test/29-05-2024-17-22-32-input-test.jsonl\n",
      "experiment.output_completed_file_path: data2/output/test/29-05-2024-17-22-32-completed-test.jsonl\n",
      "experiment.log_file: data2/output/test/29-05-2024-17-22-32-test-log.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"experiment.file_name: {experiment.file_name}\")\n",
    "print(f\"experiment.input_file_path: {experiment.input_file_path}\")\n",
    "print(f\"experiment.output_folder: {experiment.output_folder}\")\n",
    "print(f\"experiment.output_input_file_out_path: {experiment.output_input_file_out_path}\")\n",
    "print(f\"experiment.output_completed_file_path: {experiment.output_completed_file_path}\")\n",
    "print(f\"experiment.log_file: {experiment.log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the object just prints out the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test.jsonl'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{experiment}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply process a single experiment by awaiting the async `process` method (you can also use `asyncio.run` as well) .This will process all the prompts in the experiment and write the output to the output folder.\n",
    "\n",
    "The method returns the list of completed prompt dictionaries (with the response from the LLM in the \"response\" key) and a float which is the average time taken to process and wait for the response for each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending 3 queries  (attempt 1/5): 100%|██████████| 3/3 [00:03<00:00,  1.21s/query]\n",
      "Waiting for responses  (attempt 1/5): 100%|██████████| 3/3 [00:00<00:00, 255.88query/s]\n"
     ]
    }
   ],
   "source": [
    "completed_responses, avg_query_processing_time = await experiment.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that completed responses are also saved in the `completed_responses` attribute of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.completed_responses == completed_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 9,\n",
       "  'prompt': ['Hello',\n",
       "   \"My name is Bob and I'm 6 years old\",\n",
       "   'How old am I next year?'],\n",
       "  'api': 'unknown-model-name',\n",
       "  'parameters': {'candidate_count': 1,\n",
       "   'max_output_tokens': 64,\n",
       "   'temperature': 1,\n",
       "   'top_k': 40},\n",
       "  'response': 'NotImplementedError - API unknown-model-name not recognised or implemented'},\n",
       " {'id': 10,\n",
       "  'prompt': ['Can you give me a random number between 1-10?',\n",
       "   'What is +5 of that number?',\n",
       "   'What is half of that number?'],\n",
       "  'api': 'unknown-model-name',\n",
       "  'parameters': {'candidate_count': 1,\n",
       "   'max_output_tokens': 128,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 40},\n",
       "  'response': 'NotImplementedError - API unknown-model-name not recognised or implemented'},\n",
       " {'id': 11,\n",
       "  'prompt': \"How many theaters are there in London's South End?\",\n",
       "  'api': 'unknown-model-name',\n",
       "  'response': 'NotImplementedError - API unknown-model-name not recognised or implemented'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.completed_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the output, we can see we got errors that there were `NotImplementedErrors` as the model was not implemented. To see the models implemented, there is a dictionary of models in the `apis` module called `ASYNC_APIS` where the keys are the API names and the values are the corresponding classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': prompto.apis.testing.testing_api.AsyncTestAPI,\n",
       " 'azure-openai': prompto.apis.azure_openai.azure_openai.AsyncAzureOpenAIAPI,\n",
       " 'openai': prompto.apis.openai.openai.AsyncOpenAIAPI,\n",
       " 'gemini': prompto.apis.gemini.gemini.AsyncGeminiAPI,\n",
       " 'ollama': prompto.apis.ollama.ollama.AsyncOllamaAPI,\n",
       " 'huggingface-tgi': prompto.apis.huggingface_tgi.huggingface_tgi.AsyncHuggingfaceTGIAPI,\n",
       " 'quart': prompto.apis.quart.quart.AsyncQuartAPI}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompto.apis import ASYNC_APIS\n",
    "\n",
    "ASYNC_APIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Pipeline\n",
    "\n",
    "The `ExperimentPipeline` class is the main class for running the full pipeline which will continually check the input folder for new experiments to process. To initialise, it simply just takes in a `Settings` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ExperimentPipeline(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stores several things such as:\n",
    "- `settings`: `Settings` object\n",
    "- `average_per_query_processing_times`: this is a list of the average query processing times for each experiment\n",
    "- `overall_avg_proc_times`: this is a float which is an average of the values in `average_per_query_processing_times`\n",
    "\n",
    "These last two attributes are just for logging purposes to see how long each experiment takes on average and for us to give a very rough estimate of how long we may expect queries to return to us.\n",
    "\n",
    "The object will also store `experiment_files` which is a list of all the JSONL files in the input folder. When the pipeline is running, it will check this folder for new experiments to process and order then by creation time so that we process the oldest experiments first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.settings: Settings: data_folder=data2, max_queries=50, max_attempts=5, parallel=False\n",
      "Subfolders: input_folder=data2/input, output_folder=data2/output, media_folder=data2/media\n",
      "pipeline.average_per_query_processing_times: []\n",
      "pipeline.overall_avg_proc_times: 0.0\n",
      "pipeline.experiment_files: []\n"
     ]
    }
   ],
   "source": [
    "print(f\"pipeline.settings: {pipeline.settings}\")\n",
    "print(\n",
    "    f\"pipeline.average_per_query_processing_times: {pipeline.average_per_query_processing_times}\"\n",
    ")\n",
    "print(f\"pipeline.overall_avg_proc_times: {pipeline.overall_avg_proc_times}\")\n",
    "print(f\"pipeline.experiment_files: {pipeline.experiment_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key method in the `ExperimentPipeline` class is the `run()` method which will continually check the input folder for new experiments to process. When processing experiments, we create an `Experiment` object as described above, and process the experiment.\n",
    "\n",
    "You can start one from the CLI using the [`run_pipeline.py`](../../src/prompto/scripts/run_pipeline.py) script, or just use the `prompto_run_pipeline` CLI command (also see the [documentation for prompto commands](../../docs/commands.md)). This takes in several arguments:\n",
    "\n",
    "1. `--data-folder`: the path to the data folder\n",
    "2. `--max-queries`: the maximum number of queries per minute\n",
    "3. `--max-attempts`: the maximum number of attempts for each query\n",
    "4. `--parallel`: whether or not to use parallel processing\n",
    "5. `--max-queries-json`: whether or not to group prompts\n",
    "\n",
    "See the [Grouping prompts and specifying rate limits notebook](grouping_prompts_and_specifying_rate_limits.ipynb) and the [Specifying rate limits documentation](../../docs/rate_limits.md) for more information on the last two arguments.\n",
    "\n",
    "Here we will run the method to see how the pipeline runs and processes the experiments. Currently we only have one more experiment in the folder to process `\"test2.jsonl\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test2.jsonl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"data2/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 9,\n",
       "  'prompt': ['Hello',\n",
       "   \"My name is Bob and I'm 6 years old\",\n",
       "   'How old am I next year?'],\n",
       "  'api': 'test',\n",
       "  'parameters': {'candidate_count': 1,\n",
       "   'max_output_tokens': 64,\n",
       "   'temperature': 1,\n",
       "   'top_k': 40}},\n",
       " {'id': 10,\n",
       "  'prompt': ['Can you give me a random number between 1-10?',\n",
       "   'What is +5 of that number?',\n",
       "   'What is half of that number?'],\n",
       "  'api': 'test',\n",
       "  'parameters': {'candidate_count': 1,\n",
       "   'max_output_tokens': 128,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 40}},\n",
       " {'id': 11,\n",
       "  'prompt': \"How many theaters are there in London's South End?\",\n",
       "  'api': 'test'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment2 = Experiment(\"test2.jsonl\", settings=settings)\n",
    "experiment2.experiment_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `ExperimentPipeline.run()` to run the pipeline and process the experiments but we are unable to run this within a notebook as it uses `asyncio.run` which cannot be called within a notebook. However, as mentioned above, we typically would run this from the CLI using the `prompto_run_pipeline` command.\n",
    "\n",
    "In the terminal, move do this current directory (`prompto/examples/notebooks`) and run the following command:\n",
    "\n",
    "```bash\n",
    "prompto_run_pipeline --data-folder data2 --max-queries 50 --max-attempts 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the experiment has finished, check the output folder for the output file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
