{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading media to Gemini\n",
    "\n",
    "This notebook processes an experiment file and associate each media element with the id of the file when uploaded using the Files API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import base64\n",
    "import hashlib\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the location of the experiment and media\n",
    "\n",
    "experiment_location = \"data/input\"\n",
    "filename = \"gemini-multimodal-example.jsonl\"\n",
    "media_location = \"data/media\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GEMINI_API_KEY from the environment\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "if GEMINI_API_KEY is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sha256_base64(file_path, chunk_size=8192):\n",
    "    \"\"\"\n",
    "    Compute the SHA256 hash of the file at 'file_path' and return it as a base64-encoded string.\n",
    "    \"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "    return base64.b64encode(hasher.digest()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def remote_file_hash_base64(remote_file):\n",
    "    \"\"\"\n",
    "    Convert a remote file's SHA256 hash (stored as a hex-encoded UTF-8 bytes object)\n",
    "    to a base64-encoded string.\n",
    "    \"\"\"\n",
    "    hex_str = remote_file.sha256_hash.decode(\"utf-8\")\n",
    "    raw_bytes = bytes.fromhex(hex_str)\n",
    "    return base64.b64encode(raw_bytes).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def wait_for_processing(file_obj, poll_interval=10):\n",
    "    \"\"\"\n",
    "    Poll until the file is no longer in the 'PROCESSING' state.\n",
    "    Returns the updated file object.\n",
    "    \"\"\"\n",
    "    while file_obj.state.name == \"PROCESSING\":\n",
    "        print(\"Waiting for file to be processed...\")\n",
    "        time.sleep(poll_interval)\n",
    "        file_obj = genai.get_file(file_obj.name)\n",
    "    return file_obj\n",
    "\n",
    "\n",
    "def upload(file_path, already_uploaded_files):\n",
    "    \"\"\"\n",
    "    Upload the file at 'file_path' if it hasn't been uploaded yet.\n",
    "    If a file with the same SHA256 (base64-encoded) hash exists, returns its name.\n",
    "    Otherwise, uploads the file, waits for it to be processed,\n",
    "    and returns the new file's name. Raises a ValueError if processing fails.\n",
    "    \"\"\"\n",
    "    local_hash = compute_sha256_base64(file_path)\n",
    "\n",
    "    if local_hash in already_uploaded_files:\n",
    "        return already_uploaded_files[local_hash], already_uploaded_files\n",
    "\n",
    "    # Upload the file if it hasn't been found.\n",
    "    file_obj = genai.upload_file(path=file_path)\n",
    "    file_obj = wait_for_processing(file_obj)\n",
    "\n",
    "    if file_obj.state.name == \"FAILED\":\n",
    "        raise ValueError(\"File processing failed\")\n",
    "    already_uploaded_files[local_hash] = file_obj.name\n",
    "    return already_uploaded_files[local_hash], already_uploaded_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve already uploaded files\n",
    "\n",
    "uploaded_files = {\n",
    "    remote_file_hash_base64(remote_file): remote_file.name\n",
    "    for remote_file in genai.list_files()\n",
    "}\n",
    "print(f\"Found {len(uploaded_files)} files already uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_upload = set()\n",
    "experiment_path = f\"{experiment_location}/{filename}\"\n",
    "\n",
    "# Read and collect media file paths\n",
    "with open(experiment_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    data_list.append(data)\n",
    "\n",
    "    if not isinstance(data.get(\"prompt\"), list):\n",
    "        continue\n",
    "\n",
    "    files_to_upload.update(\n",
    "        f'{media_location}/{el[\"media\"]}'\n",
    "        for prompt in data[\"prompt\"]\n",
    "        for part in prompt.get(\"parts\", [])\n",
    "        if isinstance(el := part, dict) and \"media\" in el\n",
    "    )\n",
    "\n",
    "# Upload files and store mappings\n",
    "genai_files = {}\n",
    "for file_path in tqdm.tqdm(files_to_upload):\n",
    "    uploaded_filename, uploaded_files = upload(file_path, uploaded_files)\n",
    "    genai_files[file_path] = uploaded_filename\n",
    "\n",
    "# Modify data to include uploaded filenames\n",
    "for data in data_list:\n",
    "    if isinstance(data.get(\"prompt\"), list):\n",
    "        for prompt in data[\"prompt\"]:\n",
    "            for part in prompt.get(\"parts\", []):\n",
    "                if isinstance(part, dict) and \"media\" in part:\n",
    "                    file_path = f'{media_location}/{part[\"media\"]}'\n",
    "                    if file_path in genai_files:\n",
    "                        part[\"uploaded_filename\"] = genai_files[file_path]\n",
    "                    else:\n",
    "                        print(f\"Failed to find {file_path} in genai_files\")\n",
    "\n",
    "# Write modified data back to the JSONL file\n",
    "with open(experiment_path, \"w\") as f:\n",
    "    for data in data_list:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
