# Evaluation

A common use case for `prompto` is to evaluate the performance of different models on a given task where we first need to obtain a large number of responses.
In `prompto`, we provide functionality to automate the querying of different models and endpoints to obtain responses to a set of prompts and _then evaluate_ these responses.

## Automatic evaluation using an LLM-as-a-judge

## Automatic evaluation using a scoring function
